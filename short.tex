\documentclass[preprint]{article}

%% Load xspace package for proper spacing after commands
\usepackage{xspace}
%% Define the \sys command for the system name
\newcommand{\sys}{SchedCP\xspace}
%% Define the \agent command for the sched-agent name
\newcommand{\agent}{sched-agent\xspace}


\usepackage{neurips_2025}
% \usepackage[final]{neurips_2025}


\usepackage{listings}     % For ASCII-art / code blocks
\usepackage{booktabs}     % Nicer tables
\usepackage{array}        % Column types
\usepackage{tabularx}     % Automatic column width
\usepackage{enumitem}     % Compact lists



\usepackage{comment}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[english]{babel} 
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}     % For subfigures


\usepackage{hyperref}       % hyperlinks
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors




\title{Towards Agentic OS: An LLM Agent Framework for Linux Schedulers}



% \author{%
%   Yusheng Zheng$^{1}$ \quad
%   Yanpeng Hu$^{2}$ \quad
%   Andi Quinn$^{1}$ \\
%   $^{1}$UC Santa Cruz, CA, USA \quad
%   $^{2}$ShanghaiTech University, Shanghai, China \\
%   \texttt{\{yzhen165, aquinn1\}@ucsc.edu, huyp@shanghaitech.edu.cn}
% }
\sloppy
\begin{document}


\maketitle


\begin{abstract}
Operating system schedulers suffer from a fundamental semantic gap, where kernel policies fail to understand application-specific needs, leading to suboptimal performance. We introduce \sys, the first framework that enables fully autonomous Large Language Model (LLM) agents to safely and efficiently optimize Linux schedulers without human involvement. Our core insight is that the challenge is not merely to \emph{apply} a better LLM, but to architect a decoupled control plane that separates the AI's role of semantic reasoning ("what to optimize") from the system's role of execution ("how to observe and act"). Implemented as Model Context Protocol(MCP) server, \sys provides a stable interface with three key services: a Workload Analysis Engine, an evolving Scheduler Policy Repository, and an Execution Verifier that validates all AI-generated code and configure before deployment with static and dynamic analysis. 

We demonstrate this architecture's power with \agent, a multi-agent system that autonomously analyzes workloads, synthesizes custom eBPF scheduling policies, and deploys them via the sched\_ext infrastructure. Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. By bridging the semantic gap, SchedCP democratizes expert-level system optimization and represents a step towards creating truly self-optimizing, application-aware operating systems. The code will be open-sourced.
\end{abstract}



\maketitle
\section{Introduction}
\label{sec:intro}

Operating system schedulers face a fundamental challenge: kernel policies cannot understand what applications need. This semantic gap leads to suboptimal performance as Linux's EEVDF scheduler~\cite{eevdf2024} applies one-size-fits-all policies to diverse workloads. While sched\_ext~\cite{schedext2024} in Linux 6.12 enables custom extended Berkeley Packet Filter(eBPF) schedulers with safety guarantees through verification, developing them still requires both deep kernel expertise and a good understanding of the workloads.

Prior scheduler optimization approaches using reinforcement learning~\cite{mao2019decima,qiu2020firm} lack semantic understanding of workloads. While LLMs~\cite{openai2023gpt4,anthropic2024claude} and agent frameworks~\cite{autogen,geminicli,claudecode,qian2024chatdev,hong2023metagpt} have shown promise in code generation, naively applying them to scheduler development proves impractical. Our experiments show generating a basic scheduler takes 33 minutes, costs \$6, and often degrades performance. The gap remains: existing methods lack semantic understanding, while LLMs lack the scaffolding for safe, efficient, and reliable systems integration despite previous work exploring LLM for eBPF code generation~\cite{kgent}.

We introduce a decoupled architecture with two components: \sys, a control plane framework and MCP server providing safe AI-kernel interfaces with profiling, tracing and validation tools, and allow LLM to customize the kernel scheduler with eBPF; and \agent, an autonomous multi-agent system that reasons about workloads and synthesizes or configures existing optimized schedulers. This separation allows \sys\ to provide a generalizable framework for any AI agent, while \agent\ demonstrates semantic workload analysis and policy generation. Our approach reduces scheduler development from 33 minutes to an average of 2.5 minutes and costs \$0.45, making optimization viable even for short-lived workloads. We make the following contributions:

\begin{itemize}
    \item \textbf{The \sys\ interface}: A framework that exposes kernel scheduling related features via the Model Context Protocol (MCP), featuring three core services (Workload Analysis Engine, Scheduler Policy Repository, and Execution Verifier) that enable any agent to perform deep semantic analysis of workloads, do AI-driven scheduler optimization without compromising system stability, and learns from experience and improve performance over time.
    \item \textbf{\agent\ multi-agent system}: An autonomous reinforcement learning policy engine that decomposes scheduler optimization into four specialized agents (Observation, Planning, Execution, and Learning), demonstrating how LLMs can bridge the semantic gap between application requirements and kernel scheduling policies.
    \item \textbf{Evaluation}: We demonstrate that \agent\ achieves up to 1.79× performance gains on kernel compilation, 2.11× P99 latency improvement and 1.60× throughput gain on schbench, 20\% average latency reduction for batch workloads, and 13× lower cost compared to naive approaches, while maintaining system stability across diverse workloads.
\end{itemize}

\section{Motivation}
\label{sec:motivation}

We motivate our work by examining this semantic gap problem and the practical safety, performance, and cost issues revealed by our experiments.

\subsection{The Semantic Gap Problem}

The semantic gap manifests in three critical dimensions. First, a domain knowledge gap exists between developers and users: in cloud environments, DevOps engineers configuring Kubernetes lack insight into workload characteristics (latency-sensitive vs. throughput-oriented), resulting in conservative scheduling, while edge and personal device users like gamers, creative professionals, and office workers lack kernel expertise for optimization. Second, the technical complexity of scheduler development requires mastering kernel programming with lock-free structures, eBPF verification constraints, and CPU/NUMA architectures, a steep learning curve that limits innovation to few kernel experts. Third, modern workloads exhibit complex dynamic phase behavior: ML training alternates between compute-intensive forward propagation and communication-heavy gradient synchronization, web traffic varies by orders of magnitude daily, and build system parallelism changes with dependencies. LLMs can bridge these gaps by understanding high-level workload patterns from source code and deployment artifacts, translating them into concrete scheduling policies, enabling rapid automated scheduler development without human expertise, and adapting policies in real-time while operating 24/7 without breaks—providing much greater availability than human experts.

\subsection{Economic Viability of AI-Driven Scheduler Optimization}

Scheduler optimization becomes economically viable when generation costs are lower than CPU time savings. For example, with \$0.45 AI generation LLM API cost and 20\% performance improvement, a 10-hour workload on a \$2/hour instance already breaks even (\$0.45 cost vs \$4 savings). This fundamentally changes scheduler economics: traditionally, high manual engineering costs meant custom schedulers were only justified for large-scale cloud workloads running on hundreds of machines for months, but AI assistance now makes it economical to optimize even short-lived workloads like CI/CD pipelines, batch jobs, or individual builds running for just hours or days on single machines, democratizing performance optimization for previously uneconomical use cases.

\subsection{Motivation Experiment}

We tested Claude Code\cite{claudecode}, the state of the art LLM agent, with "write a FIFO scheduler in eBPF" from an empty folder, with all permissions and bash access. Of three attempts, only one succeeded. The second attempt produced pseudo-code after 6 minutes trying, and the third generated a scheduler tracer instead after 8 minutes development. The successful generation required 33 minutes, 221 LLM API calls, and 15+ iterations, costing \$6 (vs. 5 minutes typically for an expert developer). The generated code, for some workloads, exhibited poor quality with excessive overhead, performing worse than EEVDF. The agent required root access, could crash the system during testing, and lacked fallback mechanisms, which also raises safety concerns.

\subsection{Challenges in Applying LLM Agents to Schedulers}

Our experiments reveal critical challenges for AI-driven scheduler optimization, especially when fully automated: \textbf{Performance}: How do we ensure AI-generated or configured schedulers outperform existing ones rather than degrading performance? \textbf{Safety}: How do we prevent kernel crashes, soft-lockups, stalls, or starvation while maintaining stability? How can we ensure only minimal privilege needed when development and deployment? \textbf{Efficiency}: The 33-minute generation time and the \$6 cost must drop for practical deployment.
\section{The \sys\ Framework Design and Implementation}
\label{sec:schedcp_framework}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{sections/img/arch-scheddcp.pdf}
    \caption{
        \textbf{The overall architecture, showing the separation of concerns between \sys\ and \agent } 
        The \textbf{\sys} framework (bottom) acts as a safe system interface, providing tools to analyze workloads, verify code, and manage scheduler policies in the Linux kernel via eBPF.
        The \textbf{\agent} framework (top) contains the AI logic, where specialized agents for Observation, Planning, Execution, and Learning collaborate in a closed loop to autonomously create, deploy, and refine scheduling policies. The red line indicate the initialize process When \sys detects a new workload. The back arrow indicate the optimization loops, where \agent continue refines scheduler policies based on optimization plan and observation results. The Green arrows indicate the tool usage by the AI Agents.
    }
    \label{fig:frameworkarch}
\end{figure}

Our approach to agentic OS optimization is founded on a clean separation between the systems infrastructure and the AI logic, as illustrated in Figure~\ref{fig:frameworkarch}. We introduce \sys, a stable and secure control plane that acts as an 'API for OS optimization.' Our research is motivated by the insight that AI agents are fundamentally context engineering systems; like human experts, they need the right tools to gather information and act without being overwhelmed by prohibitive costs or irrelevant data. Therefore, as system researchers, our goal is not to build better AI agents, but to design superior systems and interfaces for them. \sys embodies this by providing the essential tools and safety guarantees for any agent to interact with the Linux kernel's scheduler, analogous to how an environment in reinforcement learning provides the state, actions, and rewards for an agent to learn. This section details \sys's design principles, core components, and implementation. \sys is implemented in ~4000 lines of Rust and ~6000 lines of python (Include tests).

\subsection{Design Principles}
The design of \sys\ is governed by four key principles that ensure it is safe, efficient, and future-proof. First, \textbf{decoupling and role separation} ensures that our framework remains future-proof by separating the system's role from the AI's—we decouple ``what to optimize'' (the AI's domain) from ``how to observe and act'' (the system's domain), treating the AI agent as a performance engineer using a stable set of tools provided by the system. Second, our \textbf{safety-first interface design} addresses the inherent risks of autonomous agents with kernel access by treating AI as potentially non-cautious actors and designing defensive interfaces that prevent catastrophic failures by default. Third, we \textbf{balance context and feedback} to address LLM agents' constraints from finite context windows and token costs through adaptive context provisioning, where agents start with minimal summaries and progressively request details as needed. Finally, our \textbf{composable tool architecture} follows Unix philosophy by providing atomic tools that let agents construct complex workflows through their reasoning capabilities, enabling novel solution generation rather than constraining them with rigid workflows.

\subsection{Core Components and Implementation}
\sys is engineered as a modular control plane, exposing its services to AI agents via the standard Model Context Protocol (MCP)~\cite{anthropic2024mcp}. This design cleanly separates the high-level policy orchestration managed by the agent from the low-level observation and execution handled by the framework, and avoids granting `root' privileges to the agent. The architecture consists of three primary services.

\textbf{1. Workload Analysis Engine.} This service provides agents with tiered access to system performance data. It offers three levels of information: (1) cost-effective API endpoints delivering pre-processed summaries like CPU load and memory usage, (2) secure sandbox access to basic file reading, application building, standard Linux profiling tools (\texttt{perf}, \texttt{strace}) and dynamically attachable eBPF probes for detailed analysis, and (3) a feedback channel that reports post-deployment performance metrics such as percentage change in makespan or latency. The service implements adaptive context provisioning, allowing agents to request progressively detailed information as needed.

\textbf{2. Scheduler Policy Repository.} This service is a vector database storing eBPF scheduler code with rich metadata: natural language descriptions, target workloads, and historical performance metrics. It also includes a set of executable scheduler programs. It provides APIs for semantic search and retrieval, enabling agents to find relevant schedulers or composable code primitives. To support system evolution, it includes endpoints for updating performance metrics and promoting new policies. The repository reduces generation costs by allowing reuse of proven solutions while maintaining a growing library of scheduling strategies.

\textbf{3. Execution Verifier.} This validation pipeline service provides multi-stage verification for all AI-generated code and config, beginning with the kernel's standard eBPF verifier to guarantee fundamental memory safety and termination. However, because the standard verifier is agnostic to scheduling logic, it cannot detect flaws like task starvation or unfairness; therefore, our pipeline adds a crucial second layer of scheduler-specific static analysis checkers using customize PREVAIL verifier\cite{prevail} to check for these Correctness and Logic Bugs. Code that passes both static analysis layers proceeds to dynamic validation, where it is compiled and executed within a secure micro-VM against correctness and performance tests. Upon success, the service issues a signed deployment token for a monitored canary deployment, which includes a circuit breaker to automatically revert to the last known-good scheduler if performance degrades, ensuring all policies are rigorously vetted before production use. It also ensures the \agent don't need root access to deploy eBPF schedulers.

\section{\agent: A Multi-Agent Framework for OS Optimization}
\label{sec:sched_agents}

Building on \sys, we developed \textbf{\agent}, a multi-agent AI framework for scheduler optimization. At its core, \agent\ implements in-context reinforcement learning (ICRL)\cite{incontextrl}, a paradigm where the agent adapts its strategy based on recent performance feedback in the context without costly model retraining. We realized this framework using Claude Code's subagent architecture\cite{anthropic2024subagents}, which provides specialized AI assistants with customized system prompts, tools, and separate context windows\cite{anthropic2024multiagent}. Mirroring the collaboration of expert human teams, this multi-agent structure naturally decomposes the complex optimization process into the distinct stages of the ICRL loop: observation (state), planning/execution (action), and learning (reward analysis).

To automatically trigger optimization, \sys integrates with container orchestrators and runtime like Kubernetes and Docker, enabling it to initiate the \agent's analysis cycle whenever a user deploys a new application. It can also be trigger mannually by user.

\subsection{Agent Roles and Responsibilities}

\subsubsection{Observation \& Analysis Agent - Building a Workload Profile}

The \textbf{Observation Agent} builds a comprehensive ``Workload Profile'' by strategically querying the Workload Analysis Engine. Its reasoning process determines the analysis sequence: starting with high-level summaries, then requesting deeper profiling based on initial findings. For example, after identifying a parallel build process through initial queries, the agent decides to request CPU statistics via \texttt{perf stat} and dependency traces via \texttt{strace}. The agent synthesizes these data points into a description of the workload in natural language, quantified performance characteristics, and explicit optimization goals. It manages the cost-precision tradeoff by requesting only essential information and can register for event notifications in \sys to trigger re-analysis when workload patterns change.

\subsubsection{Planning Agent - Policy Synthesis and Selection}

The \textbf{Planning Agent} transforms the Workload Profile into an optimization strategy. It constructs semantic queries for the Scheduler Policy Repository based on the profile's keywords and performance goals. The agent's decision logic follows a hierarchy: search for exact matches, broaden to similar patterns if needed, then decide among three pathways. For existing production-ready scheduler solutions with strong performance history, it configures parameters. For partial matches, it retrieves code and generates patches. When no suitable base exists, it composes new schedulers from algorithm primitives. The agent evaluates tradeoffs between reuse efficiency and customization needs using historical performance data from the repository.

\subsubsection{Execution Agent - Validated Policy Deployment}

The \textbf{Execution Agent} manages the development, validation and deployment process. It synthesizes code artifacts based on the Planning Agent's strategy, then submits them to the Execution Verifier. The agent interprets validation results and adapts accordingly: when static analysis fails, it refines the code; when dynamic tests fail, it analyzes errors and fixes logic issues. The agent decides whether to proceed, retry, or abandon approaches based on verifier feedback. Upon receiving a deployment token, it initiates canary rollout. If the circuit breaker triggers, the agent captures failure context and determines next steps, either revising the approach or escalating to the Learning Agent.

\subsubsection{Learning Agent - Performance Analysis and Knowledge Update}

The \textbf{Learning Agent} completes the in-context reinforcement learning loop and analyzes deployment outcomes to improve future performance. It retrieves metrics from the Feedback Channel and identifies success patterns and failure modes. For immediate benefit, it informs subsequent optimization cycles within the current session. For long-term improvement, it updates the Scheduler Policy Repository: refining performance metrics, annotating schedulers with deployment contexts, and promoting successful novel policies. The agent documents antipatterns from failures to prevent repetition. This dual approach enables both in-session adaptation and persistent system-wide learning.


\subsection{Example: Kernel Compilation}

To illustrate how these four agents work together, consider a kernel compilation workload. The \textbf{Observation Agent} begins by analyzing the Linux kernel source tree, executing \texttt{make -j} to understand the build process, and collecting resource usage like CPU, memory. This observation produces a Workload Profile: ``CPU-intensive parallel compilation task with short-lived processes, inter-process dependencies, and a goal to minimize makespan.'' During planning, the \textbf{Planning Agent} queries the Scheduler Policy Repository with keywords like ``throughput'' and ``compilation,'' retrieving \texttt{scx\_rusty} as a starting point. It generates a patch to make the scheduler dependency-aware. In execution, the \textbf{Execution Agent} submits the patched code to the Execution Verifier for validation, receiving a deployment token upon success. Finally, after deployment, the \textbf{Learning Agent} receives feedback that the revision achieved a 45\% reduction in makespan, contributing the improved scheduler back to the Scheduler Policy Repository for future use. This entire workflow demonstrates how \agent\ enables AI agents to autonomously optimize system performance through iterative refinement.

\section{Evaluation}
\label{sec:evaluation}

% \subsection{Research Questions}

We investigate key research questions to validate the effectiveness and efficiency of \sys: whether it can effectively configure existing schedulers (RQ1) and generate new schedulers for specific workloads (RQ2), what the cost and efficiency of scheduler generation are (RQ3), and how much \agent can continue to improve performance after its initial attempt through iterative refinement (RQ4). We evaluate \sys on two machines: an 86-core Intel Xeon 6787P with 758GB RAM running Linux 6.14, and an 8-core Intel Core Ultra 7 258V with 30GB RAM running Linux 6.13—using Claude Code (Opus 4), testing each case three times and averaging results. In all experiments, the agent successfully creates working custom scheduler configurations or eBPF programs.

\textbf{Scheduler Configuration Performance Evaluation}: For kernel compilation with tinyconfig and ``make -j 172'' on 6.14 source code (Figure~\ref{fig:performance-comparison}), \sys achieves 1.63× speedup using scx\_rusty initially, then through iterative refinement selects scx\_layered for 16\% additional gain, reaching 1.79× total improvement over EEVDF. Basic RL approaches~\cite{corbet2025ml} show no improvement, requiring costly hardware-specific retraining. On schbench~\cite{schbench2016} (Figure~\ref{fig:schbench-comparison}), while initial AI configuration (scx\_bpfland) underperformed with worse P99 latency and lower throughput, three iterations of refinement identified scx\_rusty as superior, achieving 2.11× better P99 latency and 1.60× higher throughput versus EEVDF, demonstrating effective learning from performance feedback.

\begin{figure}[h]
\centering
\begin{minipage}{0.48\textwidth}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/Linux_build_benchmark_results.pdf}
        \caption{Linux build benchmark}
        \label{fig:performance-comparison}
    \end{subfigure}
    \vspace{0.3cm}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/schbench_performance_comparison.pdf}
        \caption{Schbench P99 latency and throughput}
        \label{fig:schbench-comparison}
    \end{subfigure}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/scheduler_performance_comparison.pdf}
        \caption{Batch workloads performance}
        \label{fig:batch-performance}
    \end{subfigure}
\end{minipage}
\caption{Performance evaluation of \sys across different workloads: (a) Linux kernel compilation showing up to 1.79× speedup, (b) Schbench demonstrating 2.11× P99 latency improvement and 1.60× throughput gain through iterative optimization, and (c) AI-generated scheduler achieving 20\% average reduction in processing time for batch workloads.}
\label{fig:combined-performance}
\end{figure}


\textbf{New Scheduler Synthesis for Batch Workloads}: For 8 diverse batch workloads (e.g. file compression, video transcoding, software testing, and data analytics tasks) with long-tail distributions (40 parallel tasks: 39 short, one long), \agent correctly identified the optimization goal, workload pattern and implemented Longest Job First (LJF) scheduling, achieving 20\% average reduction in processing time (Figure~\ref{fig:batch-performance}). The cost for analysis averaged \$0.15 per workload. We note that the powerful Claude Opus agent successfully classified and analysis all 8 workloads, whereas the smaller Claude Sonnet model could not. Generation costs also improved from our initial experiments: time reduced 13× (33 to 2.5 minutes) and cost dropped from \$6 to \$0.45 per workload, demonstrating both performance gains and economic viability.


\section{Discussion}
\label{sec:discussion}

While prior ML approaches to system optimization, including learned indexes~\cite{kraska2018learned}, database tuning~\cite{marcus2019neo,vanaken2017ottertune}, and RL-based schedulers~\cite{mao2019decima,qiu2020firm,zhang2024mrsch,mao2019park}, require extensive training and lack semantic understanding to transfer across workloads, and recent LLM work focused on diagnostics~\cite{wang2024llmsys} and code generation~\cite{wei2024mapper,10.1145/3672197.3673434}, ours is the first to enable autonomous agents to design, configure, and generate kernel schedulers for end-to-end optimization. By leveraging LLM reasoning with sched\_ext and eBPF, \sys uniquely bridges the semantic gap between applications and system policy. Looking forward, extending our framework beyond schedulers to cache policies, DVFS, network configuration, and sysctl parameters presents opportunities for unified OS optimization, where cross-component decisions (CPU, memory, I/O, power) could unlock further gains through new inter-component dependency abstractions, enabling self-optimizing operating systems that democratize expert-level performance.

\bibliographystyle{plain}
\bibliography{sample-base}






\end{document}
