\section{Introduction}

Efficient scheduling is critical for performance in multi-tenant HPC clusters and cloud datacenters, where jobs compete for CPUs, GPUs, memory, and I/O resources. Traditional schedulers (e.g. SLURM, PBS, Torque) assume jobs have fixed resource demands and durations. For example, Slurm's batch scheduler allocates resources based on user-specified estimates, leading to long queues and low utilization when workloads vary unexpectedly~\cite{arxiv2401}. In contrast, modern cloud schedulers (e.g. Kubernetes, Borg) allow elastic scaling, but HPC environments lack such dynamic adaptability, often reporting dozens of minutes of average wait time for jobs and low overall resource utilization~\cite{arxiv2401}. Meanwhile, emerging AI-powered workloads (e.g. large-scale training, inference chains) and heterogeneous architectures demand multi-objective scheduling (latency vs throughput vs energy) and fine-grained resource awareness. Manually designing scheduling heuristics or tuning parameters for each scenario is prohibitively expensive and error-prone.

Recent research has explored machine learning for scheduling. Reinforcement learning (RL) agents have been shown to learn scheduling policies directly from job traces. For instance, \emph{Decima} uses RL on graph neural networks to schedule batch jobs in data clusters, improving throughput and makespan over heuristic baselines. \emph{DRAS} employs deep RL for HPC batch scheduling with job reservation and backfilling, achieving up to 45\% reduction in job completion time versus static policies. Other works combine graph neural nets and RL for DAG and resource co-scheduling, yielding significant utilization gains. However, RL approaches require extensive training for each environment and may struggle with new workload patterns or multiple scheduling objectives.

Meanwhile, LLM-based agents have demonstrated remarkable reasoning abilities in complex tasks. Even under zero-shot prompting, LLMs can produce reasonable initial solutions for scheduling problems. For example, an LLM (e.g. GPT-4) can cluster and assign conference papers to sessions, creating draft schedules that are often only a few adjustments away from human quality. In HPC scheduling, recent work shows that an LLM can reason through multi-objective scheduling scenarios (minimize makespan, wait time, etc.) using a ReAct (reason+act) framework, balancing goals without domain-specific training. These results suggest that LLMs can generalize scheduling knowledge from text and math contexts to new workloads.

Our insight is that an LLM agent can automatically analyze a workload and generate a custom scheduler in a high-level form, which is then enacted or fine-tuned by RL. By treating scheduling as an AI planning problem, \sys{} can leverage chain-of-thought reasoning and vast pretraining to capture domain heuristics, while RL handles environment-specific optimization. Concretely, the \sys{} workflow is: (1) \emph{Workload Analysis}: the LLM reads a description of the tasks (program code snippets, performance traces, system constraints) and identifies key properties (e.g. task DAG structure, compute vs I/O intensity, real-time demands). (2) \emph{Policy Synthesis}: using a scheduling DSL, the agent generates a candidate scheduling policy or parameter configuration. (3) \emph{RL Fine-tuning}: an RL loop simulates the proposed scheduler on the workload; performance feedback is used to refine the policy (e.g. adjusting priorities or parameters). This closed loop continues until convergence. Crucially, the LLM's plan serves as a strong initialization that guides RL, requiring far fewer iterations than blind search.

\sys{}'s generality and automatic nature address several challenges:

\begin{itemize}
\item \emph{Diverse Workloads}: By analyzing workload semantics, the LLM tailors scheduling to each scenario, from CPU-bound compute jobs to I/O-heavy pipelines, without manual reprogramming. We evaluate on a broad suite of benchmarks (HPC linear algebra DAGs, multi-stage dataflows, LLM inference jobs) to test generalization.
\item \emph{Complex Objectives}: The system can incorporate multi-objective goals (throughput, latency, fairness) into the agent prompt, letting the LLM reason about trade-offs.
\item \emph{Explainability}: The LLM can produce human-readable justifications for scheduling decisions (via chain-of-thought), aiding debugging and trust in mission-critical settings.
\item \emph{DSL Integration}: We define a small domain-specific language for scheduling policies (inspired by prior scheduler DSLs). The LLM outputs code or configuration in this DSL, which is then compiled into the system. The DSL abstracts low-level details while remaining expressive enough to capture policies.
\end{itemize}

In summary, we contribute: (1) The design of \emph{\sys{}}, a new framework combining LLM reasoning with reinforcement learning to generate dynamic schedulers. (2) A scheduling DSL and Agent-System interface that enables the LLM to emit executable policies while simplifying RL tuning. (3) A comprehensive evaluation on varied workloads showing \sys{} improves scheduling performance over static heuristics and pure-RL baselines, and generalizes across tasks.