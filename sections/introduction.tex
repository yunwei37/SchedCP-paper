\section{Introduction}

Operating system schedulers face a fundamental challenge: kernel policies cannot understand what applications need. This semantic gap leads to suboptimal performance across modern computing infrastructure. In cloud platforms, system administrators who manage schedulers are not the developers who understand application behavior. On personal devices, users lack the kernel expertise to optimize their systems for gaming or creative workloads. Meanwhile, workloads themselves are increasingly dynamic—a machine learning training job may shift from compute-intensive to I/O-bound phases, requiring different scheduling strategies that no human can adjust in real-time.

Recent advances in reinforcement learning have attempted to address scheduler optimization through automated parameter tuning. Systems like Decima~\cite{mao2019decima} and Firm~\cite{qiu2020firm} have shown promise in specific domains. However, these approaches remain fundamentally limited: they cannot understand application-level requirements such as whether a workload is latency-critical or throughput-oriented. They miss optimization opportunities that require semantic understanding—for instance, prioritizing compilation tasks based on code dependencies could significantly accelerate software builds, but no existing system can automatically discover and implement such strategies.

The rapid advancement of Large Language Model (LLM) agents in 2024-2025 presents an unprecedented opportunity. Modern LLM agents can understand complex system behaviors, generate code, and reason about optimization strategies. However, our motivation experiments reveal significant challenges: automatically generating a basic FIFO scheduler from scratch required 33 minutes, 221 API calls with Claude Code through many trial-and-error iterations, costing \textasciitilde\$6, and sometimes the generated code degraded system performance. This highlights the need for carefully designed interfaces that balance AI capabilities with practical constraints. We propose treating these AI agents as expert system administrators—but with appropriate safety constraints and interfaces designed for their unique capabilities and limitations.

This paper presents the first framework for using fully automatic LLM agents to dynamically optimize Linux schedulers. Our framework can be leveraged by any AI agent, from open-source Gemini CLI to proprietary agents like Claude Code. A key insight is that LLM agents operate on the control plane, not the data plane—while scheduling decisions occur at microsecond timescales in the kernel, workload patterns change at minute-to-hour timescales, well-suited for LLM optimization. Our system enables AI agents to select, configure, modify, or generate entirely new scheduling algorithms tailored to specific workloads. Built on the production-ready sched\_ext infrastructure, our framework maintains a self-evolving library of schedulers that grows and improves through experience.

To achieve this vision while addressing the challenges revealed in our experiments, our design is guided by four key principles derived from treating AI agents as context engineering systems and similar to human experts: (1) \textbf{Decoupling} the ``what to optimize'' (AI's domain) from ``how to observe and act'' (system's domain); (2) \textbf{Context Balance} to provide sufficient information for decisions while controlling costs, and give feedback earlier; (3) \textbf{Composable Tools} that leverage LLM Agent's dynamic planning, code generation and tool usage capabilities; and (4) \textbf{Safety-First Design} that treats AI as potentially non-cautious actors requiring defensive interfaces. These principles ensure our system remains effective as AI models evolve while preventing catastrophic failures. We think these principles also are generalizable to other domains and systems.

Following these principles, we implement a framework with five core components: (1) static analysis and testing for safe code deployment, (2) scheduler libraries with reusable optimization patterns, (3) reinforcement learning for continuous improvement, (4) profiling tools for workload characterization, and (5) a unified interface enabling any LLM to optimize schedulers. This modular design evolves with AI capabilities without system redesign. Once generated, schedulers execute as native eBPF code with no LLM overhead in the critical path, achieving up to 80\% speedup for Linux kernel builds and 50\% latency reduction for scheduler benchmarks.

We make the following contributions:
\begin{itemize}
\item Design and implementation of the first comprehensive LLM agent framework for OS scheduler optimization
\item A modular extension system that can be leveraged by any AI agent, from open-source to proprietary models
\item A set of design principles for AI-system interfaces that balance capability, cost, and safety
\item A self-evolving scheduler library that grows through experience and improves with model capabilities
\item Evaluation demonstrating 30-80\% performance improvements across diverse workloads
\item Discussion on the challenges and future directions of AI-driven system optimizations
\end{itemize}

The remainder of this paper is organized as follows. Section II provides background on scheduler evolution and LLM capabilities. Section III details our motivation through concrete experiments. Section IV presents our system design. Section V details implementation. Section VI evaluates performance across multiple workloads. Section VII discusses related work, Section VIII presents future work and impact, and Section IX concludes.