\section{Conclusion}

This paper presents the first comprehensive framework for using fully automatic LLM agents to dynamically optimize Linux schedulers, bridging the semantic gap between application needs and kernel scheduler capabilities. Our modular design enables any AI agent—from open-source models to proprietary systems—to generate optimized scheduling algorithms automatically. The framework's core principles of decoupling AI and system responsibilities, maintaining context balance, providing composable tools, and enforcing safety-first design establish a blueprint that extends beyond schedulers to any AI-system interface. Through extensive evaluation across diverse workloads from kernel compilation to latency-sensitive services, we demonstrate performance improvements of 30-80\% while reducing optimization costs by 85-88\% compared to naive approaches. The framework operates effectively from edge devices to 256-core servers, proving its real-world applicability.

Beyond immediate technical achievements, this work fundamentally democratizes system optimization. Expert-level performance, previously requiring specialized kernel engineering teams, becomes accessible to anyone who can describe workload requirements in natural language. Cloud operators can optimize for specific service mixes, game developers can tune for graphics workloads, and researchers can experiment with novel strategies—all without kernel expertise. The self-evolving scheduler library represents a paradigm shift where system software continuously learns and improves rather than remaining static between manual updates. Each workload encountered adds to the accumulated knowledge, creating systems that get better at their jobs simply by doing them.

We envision a future where operating systems adapt perfectly to each application's needs, replacing one-size-fits-all policies with dynamic optimization. The principles we establish generalize to memory management, I/O scheduling, power management, and beyond. As AI capabilities advance with multi-modal understanding and longer context windows, the framework can leverage these improvements immediately through its modular architecture. This work demonstrates that AI-driven OS optimization is not just possible but practical and economically viable. The era of static operating systems is ending—we are entering an age of intelligent, self-evolving systems that continuously optimize themselves for the workloads they serve.

\section{Future Work}

Our framework's architecture naturally extends beyond CPU scheduling to other system components that would benefit from workload-specific optimization. Memory management presents immediate opportunities where AI agents could generate custom page replacement policies understanding application access patterns, while NUMA optimization could leverage AI's ability to comprehend complex communication patterns. I/O scheduling for modern NVMe SSDs with multiple queues and computational storage creates an optimization space that traditional schedulers cannot fully exploit. Power management through intelligent DVFS policies that understand application phase behavior could yield significant energy savings. The most exciting direction involves cross-component optimization—coordinating CPU, memory, I/O, and power decisions holistically. For example, memory-intensive workloads could benefit from joint CPU-memory scheduling that ensures threads with complementary access patterns run simultaneously to maximize bandwidth utilization.

Critical research challenges remain in establishing theoretical foundations for AI-driven optimization. Formal verification of AI-generated schedulers could prove properties like fairness and liveness beyond the basic safety guarantees currently provided by eBPF verification. Understanding performance improvement bounds would help identify where optimization efforts provide maximum value versus diminishing returns. The rapid evolution of AI capabilities—particularly multi-modal understanding that could analyze code, performance graphs, and logs simultaneously—promises dramatic improvements in optimization effectiveness. Longer context windows approaching millions of tokens will enable analysis of entire codebases and months of performance history, transforming local optimizations into global system improvements.

Building trust in AI-generated kernel code requires extensive testing, clear audit trails, and gradual deployment strategies. Every scheduler generation must include detailed reasoning logs and human-readable documentation explaining design decisions. Ethical considerations around fairness in resource allocation and privacy in workload analysis need careful attention as AI systems make increasingly sophisticated scheduling decisions. We call on the community to contribute to our open-source implementation, share workload traces to improve AI training, develop new safety mechanisms, and explore applications to other system components. The transformation from static to self-evolving operating systems requires collective effort, but the potential rewards—democratized optimization and continuously improving performance—justify this fundamental shift in how we build system software.