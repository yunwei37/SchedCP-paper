\section{Conclusion}

We present the first framework enabling LLM agents to automatically optimize Linux schedulers, achieving 30-80\% performance improvements while reducing costs by 85-88\%. Our modular design democratizes system optimizationâ€”anyone can describe workload requirements in natural language to obtain expert-level performance previously requiring kernel engineering teams. The self-evolving scheduler library continuously improves through accumulated knowledge, shifting from static software to learning systems. Built on principles of decoupling AI and system responsibilities, maintaining context balance, providing composable tools, and enforcing safety, our framework extends beyond CPU scheduling to memory management, I/O scheduling, and power optimization. This work proves AI-driven OS optimization is practical and economically viable, marking the transition from static operating systems to intelligent, self-evolving systems that optimize for their workloads.

\section{Future Work}

Our framework extends to memory management with custom page replacement policies, NUMA optimization, I/O scheduling for NVMe SSDs, and power management through intelligent DVFS policies. Cross-component optimization coordinating CPU, memory, I/O, and power decisions holistically presents significant opportunities. Research challenges include formal verification of AI-generated schedulers, understanding performance improvement bounds, and leveraging AI advances like multi-modal understanding and million-token context windows. Building trust requires comprehensive testing, audit trails, and addressing ethical considerations around fairness and privacy. We encourage community contributions to our open-source implementation, workload trace sharing for AI training, and exploration of applications beyond CPU scheduling as we transform static operating systems into intelligent, self-evolving infrastructure.