\section{Conclusion}

This paper presents the first framework for automatic LLM agents to optimize Linux schedulers, bridging the gap between application needs and kernel capabilities. Our modular design enables any AI agent—open-source or proprietary—to generate optimized scheduling algorithms. The framework's principles of decoupling AI and system responsibilities, maintaining context balance, providing composable tools, and enforcing safety establish a blueprint extending beyond schedulers to any AI-system interface. Evaluation across workloads from kernel compilation to latency-sensitive services shows performance improvements of 30-80\% while reducing costs by 85-88\% compared to naive approaches. The framework operates from edge devices to 256-core servers, proving real-world applicability.

This work democratizes system optimization. Expert-level performance, previously requiring kernel engineering teams, becomes accessible to anyone describing workload requirements in natural language. Cloud operators optimize for service mixes, game developers tune for graphics workloads, and researchers experiment with strategies—without kernel expertise. The self-evolving scheduler library shifts from static software to continuous learning and improvement. Each workload adds to accumulated knowledge, creating systems that improve by doing their jobs.

We envision operating systems adapting to each application's needs, replacing one-size-fits-all policies with dynamic optimization. Our principles generalize to memory management, I/O scheduling, power management, and beyond. As AI advances with multi-modal understanding and longer context windows, the framework leverages improvements through modular architecture. This work shows AI-driven OS optimization is practical and economically viable. Static operating systems are ending—we enter an age of intelligent, self-evolving systems optimizing for their workloads.

\section{Future Work}

Our framework extends beyond CPU scheduling to system components benefiting from workload-specific optimization. Memory management offers opportunities for AI agents generating custom page replacement policies understanding access patterns, while NUMA optimization leverages AI understanding of communication patterns. I/O scheduling for NVMe SSDs with multiple queues and computational storage creates optimization space traditional schedulers cannot exploit. Power management through intelligent DVFS policies understanding application phases yields energy savings. Cross-component optimization coordinates CPU, memory, I/O, and power decisions holistically. Memory-intensive workloads benefit from joint CPU-memory scheduling ensuring threads with complementary access patterns run simultaneously, maximizing bandwidth.

Research challenges remain in establishing theoretical foundations for AI-driven optimization. Formal verification of AI-generated schedulers could prove fairness and liveness beyond eBPF's basic safety guarantees. Understanding performance improvement bounds identifies where optimization provides maximum value versus diminishing returns. AI evolution—particularly multi-modal understanding analyzing code, performance graphs, and logs simultaneously—promises improved optimization effectiveness. Longer context windows approaching millions of tokens enable analyzing entire codebases and months of performance history, transforming local optimizations into global improvements.

Building trust in AI-generated kernel code requires testing, audit trails, and gradual deployment. Scheduler generation must include reasoning logs and documentation explaining design decisions. Ethical considerations around resource allocation fairness and workload analysis privacy need attention as AI makes sophisticated scheduling decisions. We call on the community to contribute to our open-source implementation, share workload traces for AI training, develop safety mechanisms, and explore applications to other components. Transforming from static to self-evolving operating systems requires collective effort, but democratized optimization and continuous performance improvement justify this shift in building system software.