\section{Related Work}

\emph{Batch and Cluster Scheduling:} Traditional HPC schedulers (e.g. Slurm, Torque) use heuristic policies like FCFS with backfilling. These are simple and fast but inflexible. Heterogeneity-aware schedulers like \emph{Gavel} (OSDI'20) recast policies as optimization problems and adapt allocations to hardware differences, but still rely on fixed objectives.

\emph{RL-based Scheduling:} Recent works apply reinforcement learning to scheduling. Decima (SIGMETRICS'17) uses graph neural nets for DAG job scheduling in data centers. RLScheduler (SC'20) learns HPC batch scheduling policies from traces. DRAS (IPDPS'21) introduces a hierarchical deep RL agent for HPC, incorporating HPC-specific features like job reservation and backfilling. These achieve up to ~45\% performance gains over heuristics. MRSch (arXiv'24) uses \emph{multi-objective RL} (direct future prediction) to schedule with multiple resources (CPU, I/O, burst buffer), improving performance by up to 48\%. All these highlight RL's ability to adapt but require extensive training and are specialized per workload. In contrast, \sys{} does not need workload-specific retraining; the LLM jumpstarts the policy.

\emph{Task-based Runtime Scheduling:} Systems like StarPU, Legion, and OpenMP runtimes schedule fine-grained tasks via list scheduling and priorities~\cite{arxiv2404}. INSPIRIT (arXiv'24) improves such schedulers by introducing new task attributes (``inspiring ability/efficiency'') to guide priorities without domain knowledge~\cite{arxiv2404b}. Task scheduling works typically need domain-specific tuning~\cite{arxiv2404c}. \sys{} targets this problem by having the LLM deduce useful priority rules from workload semantics.

\emph{Multi-Objective and Online Schedulers:} Schedulers that handle multiple goals or online adaptation include Cilantro (OSDI'23), which continuously learns performance-to-resource mappings to meet arbitrary objectives, and RL-based multi-resource schemes. These systems demonstrate the importance of adaptivity; \sys{} aims to incorporate such capabilities via the agent's ability to reconfigure policies online.

\emph{LLM and Auto-Tuning in Systems:} The use of large language models in systems control is emerging. Jobson \& Li (AIware'24) showed LLMs can draft conference schedules. Zhang et al. (ICASSP'25) use an LLM to optimize parallel program mappers via a DSL interface, achieving 3.8$\times$ speedups over OpenTuner. Their \emph{Agent-System Interface} inspires our DSL usage: abstract low-level code into a structured language, letting the LLM explore high-level mappings. In OS and cluster contexts, LLM-based agent operating systems (e.g. AIOS~\cite{arxiv2403}) propose schedulers that prioritize LLM tool calls. \sys{} builds on this vision, using an LLM itself as the scheduling agent.

\emph{Autotuning Frameworks:} AutoSched (ICS'24) tunes GPU-training schedulers by generating synthetic workloads and searching config space~\cite{tianweiz07,tianweiz07b}, improving schedulers by ~46\%. \sys{} is related but more general: instead of hand-engineered search, it uses an LLM for initial policy and RL for tuning, applicable beyond DL workloads. Prior auto-tuners (e.g. ytopt) optimize code or DBMS knobs, but \sys{} uniquely targets full scheduler logic synthesis.

In summary, \sys{} differs by combining \emph{LLM reasoning} with \emph{RL fine-tuning} to generate scheduler logic. It is the first system (to our knowledge) that leverages an LLM agent to both analyze workloads and emit scheduling policies in a DSL, bridging human insight and automated learning.