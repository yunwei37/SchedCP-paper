\section{Related Work}

\textbf{RL-based Scheduling Limitations.} While Decima~\cite{mao2019decima} and Firm~\cite{qiu2020firm} demonstrate RL's potential for scheduling, they face critical limitations. Decima requires millions of scheduling decisions before converging and cannot transfer policies between workloads. Recent advances like MrSch~\cite{zhang2024mrsch} achieve up to 48\% performance improvements but remain black boxes without decision explanations. Most critically, RL methods miss semantic optimization opportunities—they see only task durations and dependencies, not that a workload represents compilation where critical path prioritization applies. Our approach differs fundamentally: LLMs apply knowledge from training on vast code and documentation corpora, enabling immediate semantic understanding without extensive trial-and-error learning.

\textbf{LLMs in Systems.} Recent work demonstrates LLMs' systems understanding—Wang et al.~\cite{wang2024llmsys} show GPT-4 correctly diagnoses 78\% of distributed system performance problems given logs and metrics. Mapper generation~\cite{wei2024mapper} represents the closest prior work, using LLMs to optimize parallel program execution on heterogeneous hardware with 3.8× average speedups. However, mapper generation operates in a constrained domain with well-defined parallel patterns and clear performance models. We extend to kernel schedulers handling arbitrary workloads with complex behaviors, stricter safety requirements, and subtle performance implications. We are the first to demonstrate AI agents generating production-quality kernel schedulers from natural language requirements.

\textbf{Key Distinctions.} Our work uniquely combines semantic understanding with production readiness. Unlike pure RL approaches that learn from scratch, we leverage LLMs' training to understand workload intent and apply known optimizations. Unlike auto-tuners like OtterTune~\cite{vanaken2017ottertune} that optimize existing parameters, we generate entirely new scheduling algorithms tailored to workloads. Unlike coding assistants that help developers, we create an autonomous optimization loop with continuous feedback and improvement. Our self-evolving scheduler library accumulates knowledge across workloads, fundamentally different from static ML models. Built on sched\_ext with eBPF safety guarantees, our generated schedulers run in production Linux systems—a critical advance over research prototypes.