\section{Background}

\subsection{Evolution of Linux Schedulers}

\subsubsection{Traditional Linux Schedulers}

The Linux kernel has undergone significant evolution in its scheduling subsystem over the past decades. The Completely Fair Scheduler (CFS), introduced in Linux 2.6.23, represents the current default scheduling algorithm and embodies a philosophy of providing "ideal, precise multi-tasking" to all runnable tasks~\cite{wong2008cfs}. CFS models an ideal processor that can run multiple tasks simultaneously, giving each task an equal share of processor time. To approximate this ideal on real hardware, CFS maintains a red-black tree of runnable tasks sorted by their "virtual runtime," ensuring that tasks with the least CPU time are scheduled first.

Beyond CFS, Linux provides real-time scheduling classes including SCHED\_FIFO and SCHED\_RR, which offer deterministic behavior for time-critical applications. These schedulers operate with fixed priorities and preempt normal tasks, making them suitable for embedded systems and real-time applications. However, all these traditional schedulers share a fundamental limitation: they implement one-size-fits-all policies that cannot adapt to specific application requirements or workload characteristics. This inflexibility becomes increasingly problematic as modern computing environments host diverse workloads ranging from latency-sensitive web services to throughput-oriented batch processing jobs.

\subsubsection{sched\_ext (Scheduler Extensions)}

The recent introduction of sched\_ext in Linux 6.12 marks a paradigm shift in kernel scheduler development~\cite{schedext2024}. This framework enables developers to implement custom schedulers as BPF programs that can be dynamically loaded into the kernel without requiring kernel modifications or reboots. The significance of this advancement cannot be overstated: for the first time in Linux history, scheduler development can proceed at user-space velocity while maintaining kernel-level performance and safety.

The sched\_ext framework provides a comprehensive set of callbacks that allow BPF schedulers to control all aspects of task scheduling. These include task enqueueing, CPU selection, load balancing, and idle CPU management. The framework ensures production readiness through minimal overhead—typically less than 1\% in real-world workloads—while providing safety guarantees through BPF verification. This combination of flexibility, safety, and performance makes sched\_ext an ideal platform for experimenting with novel scheduling algorithms, including those generated by AI agents.

\subsubsection{eBPF Technology}

Extended Berkeley Packet Filter (eBPF) has evolved from its origins as a packet filtering mechanism into a general-purpose in-kernel virtual machine that enables safe execution of user-provided programs in kernel space~\cite{mccanne1993bpf,gregg2019bpf}. The eBPF verifier performs extensive static analysis to ensure that programs cannot crash the kernel, access invalid memory, or enter infinite loops. This verification process examines all possible execution paths and enforces strict bounds on loop iterations and memory accesses.

The Just-In-Time (JIT) compilation of eBPF bytecode to native machine instructions ensures near-native performance, making eBPF suitable for performance-critical paths like scheduling. The technology has already transformed multiple kernel subsystems including networking, tracing, and security. Its application to scheduling through sched\_ext represents a natural evolution that brings the same benefits of programmability and safety to the scheduler subsystem. The rich ecosystem of eBPF tools, including bpftool, libbpf, and various tracing utilities, provides a mature foundation for developing and debugging scheduler implementations.

\subsubsection{Infrastructure Challenges}

Modern computing infrastructure faces unprecedented scheduling challenges that traditional approaches cannot adequately address. In cloud environments, the separation between system administrators who configure schedulers and application developers who understand workload requirements creates a fundamental knowledge gap. System administrators must optimize for diverse workloads without deep understanding of application behavior, while developers lack the kernel expertise to influence scheduling decisions.

The heterogeneity of modern workloads further complicates scheduling. A single machine might simultaneously run latency-critical web services requiring microsecond-level response times, batch processing jobs optimizing for throughput, and machine learning workloads alternating between compute-intensive and communication-intensive phases. Static scheduling policies cannot adapt to these dynamic requirements, leading to suboptimal performance and resource utilization. Studies show that many HPC centers achieve less than 50\% utilization due to conservative scheduling policies~\cite{feitelson2023utilization}, while cloud providers report similar inefficiencies in resource allocation.

\subsection{AI/ML in Systems Optimization}

\subsubsection{Reinforcement Learning for OS}

The application of reinforcement learning to operating system optimization has gained significant momentum in recent years, with several high-profile successes demonstrating the potential of learned policies. Decima~\cite{mao2019decima}, presented at SIGCOMM 2019, pioneered the use of graph neural networks for datacenter job scheduling, learning to optimize job completion times by considering complex dependencies between tasks. The system demonstrated improvements of up to 40\% in average job completion time compared to traditional heuristics.

Firm~\cite{qiu2020firm}, published at OSDI 2020, extended RL techniques to multi-resource cluster management with fairness constraints. By modeling the cluster state as a multi-agent system, Firm learns policies that balance resource efficiency with fairness guarantees. More recent work like Park~\cite{mao2019park} provides a platform for applying RL to various computer systems problems, demonstrating the generality of the approach.

However, these RL-based systems face fundamental limitations that motivate our work. First, they require extensive training for each specific workload or environment, often needing millions of scheduling decisions to converge to good policies. Second, they cannot leverage domain knowledge or understand application semantics—they operate purely on statistical patterns in the data. Third, the learned policies are typically black boxes that provide no explanation for their decisions, making debugging and trust difficult. Most critically, they miss optimization opportunities that require semantic understanding, such as recognizing that certain tasks in a compilation pipeline can be prioritized based on dependency analysis.

\subsubsection{LLMs for Code Generation}

The emergence of large language models has revolutionized code synthesis, with systems like Codex~\cite{chen2021codex} (powering GitHub Copilot), GPT-4~\cite{openai2023gpt4}, and Claude~\cite{anthropic2024claude} demonstrating remarkable abilities to generate complex systems code from natural language descriptions. These models have been trained on vast corpora including documentation, code repositories, and technical discussions, giving them deep understanding of programming patterns and system design principles.

Recent evaluations show that LLMs can successfully generate various types of systems code, from device drivers to network protocols. Their ability to understand context and leverage learned patterns makes them particularly suitable for tasks that require domain knowledge, such as implementing scheduling policies for specific workload types. Unlike traditional program synthesis approaches that rely on formal specifications, LLMs can work from informal descriptions and examples, making them more accessible to non-experts.

The key capabilities that make LLMs suitable for scheduler generation include understanding natural language specifications of performance requirements, generating syntactically and semantically correct systems code, reasoning about performance trade-offs and optimization strategies, and learning from examples in documentation and existing code. These capabilities address precisely the limitations of RL-based approaches, suggesting that combining LLMs with RL could yield superior results.

\subsubsection{AI Agents and Autonomous Systems}

The year 2024 marked an inflection point in AI agent capabilities, with the emergence of fully autonomous coding agents that can understand requirements, generate code, test implementations, and iterate based on feedback. Claude Code represents a prime example of this new generation, capable of autonomously developing complex software projects with minimal human intervention. These agents go beyond simple code completion to engage in genuine software engineering tasks including design, implementation, debugging, and optimization.

GitHub Copilot Workspace extends this concept to repository-wide changes, enabling AI agents to understand entire codebases and make coordinated modifications across multiple files. Devin demonstrates end-to-end software engineering capabilities, from understanding requirements through deployment. These systems combine multiple AI capabilities including natural language understanding, code generation, test synthesis, and error correction into cohesive agents that can tackle complex programming tasks.

The evolution from simple code completion to autonomous agents opens unprecedented opportunities for system optimization. Unlike traditional tools that assist human developers, these agents can work continuously to optimize systems, explore design spaces that humans might not consider, and accumulate knowledge across multiple optimization tasks. Their ability to understand both high-level requirements and low-level implementation details makes them ideal candidates for bridging the semantic gap in scheduler optimization that has long plagued traditional approaches.