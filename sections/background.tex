\section{Background}
\label{sec:background}

\subsection{Linux Scheduling and sched\_ext}

Linux's default CFS (Completely Fair Scheduler)~\cite{wong2008cfs} implements a one-size-fits-all policy but is unoptimized for diverse workloads ranging from latency-sensitive web services to throughput-oriented batch jobs. While CFS ensures fairness through virtual runtime, modern systems require adaptive scheduling. sched\_ext~\cite{schedext2024}, introduced in Linux 6.12, enables dynamic loading of custom schedulers as eBPF programs without kernel modifications. Built on eBPF~\cite{mccanne1993bpf,gregg2019bpf}, which evolved from packet filtering into a general-purpose in-kernel VM, sched\_ext provides hooks for task enqueueing, CPU selection, load balancing, and idle management. The eBPF verifier ensures safety through static analysis, preventing crashes, invalid memory access, and infinite loops. Production schedulers like scx\_rusty (work-stealing), scx\_layered (hierarchical), and scx\_central (NUMA-aware) demonstrate eBPF can implement complex algorithms matching traditional schedulers with minimal overhead (<1\%).

\subsection{LLMs and Autonomous Agents}

Large language models (LLMs) have transformed software development. Models such as OpenAI’s Codex~\cite{chen2021codex}, GPT-4~\cite{openai2023gpt4}, and Anthropic’s Claude~\cite{anthropic2024claude} can generate complex code from natural language descriptions, automating tasks from code synthesis to debugging. Empirical studies report that GitHub Copilot  helps developers complete coding tasks over 50\% faster on average~\cite{peng2023impact}. 
LLMs have begun to contribute not to development but to system maintenance, such as troubleshooting distributed systems\cite{de2025llm}.
We also have seen a shift from using LLMs purely as code assistants to deploying autonomous LLM-based agents that carry out end-to-end software engineering workflows in 2024-2025. Emerging agent systems like Anthropic’s Claude Code, GitHub’s Copilot Workspace, and Cognition’s Devin aim to handle the entire development cycle — from interpreting requirements and architecting solutions to implementing multi-file code changes, testing, and debugging — with minimal human intervention~\cite{dohmke2024copilotworkspace,sharma2024devin}. Multi-agent frameworks are also being explored, where multiple specialized LLM agents collaborate on software projects~\cite{qian2024chatdev,hong2023metagpt}. However, even the most advanced AI coding tools today primarily serve as developer aids rather than autonomously optimizing low-level system components. This gap suggests an opportunity to leverage LLMs’ semantic understanding to bridge high-level application needs with underlying system-level optimizations. 