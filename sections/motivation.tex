\section{Motivation and Challenges}

High-performance systems face stochastic and evolving workloads. Jobs arrive with uncertain run times, memory footprints, and interdependencies. Static schedulers (FCFS, fixed-priority, hand-tuned backfilling) cannot adapt in real time. For example, Horizon2022 reports that many HPC centers have less than 50\% utilization due to fixed allocations and batch wait time. In contrast, cloud clusters use autoscaling and elastic scheduling to handle variability~\cite{arxiv2401}. There is thus a pressing need for automated scheduler adaptation.

While RL has shown promise, it alone faces challenges in our setting. RL agents need many simulated jobs to train and often overfit to specific workload distributions. The state and action spaces in scheduling are huge (all pending jobs $\times$ possible allocations), leading to sample inefficiency. Moreover, RL typically outputs opaque neural policies with no rationale. On the other hand, LLMs bring prior knowledge from vast corpora, including domain heuristics and system documentation. They can ``understand'' a workload description and propose strategies in natural language or code. Recent work on RL scheduler DSLs shows the power of structured policy representations. Our key observation is that LLMs can kick-start scheduling design, reducing RL training burden, while RL can fine-tune details (like numeric parameters).

However, combining LLMs and RL for scheduling raises new challenges:

\begin{itemize}
\item \emph{Workload Description}: What input to give the LLM? We provide a concise summary of workload features (e.g. job DAG topology, average CPU vs IO usage, deadlines) in natural language or JSON. The LLM must parse this and map it to scheduling actions.
\item \emph{Action Representation}: The LLM must output a scheduler policy in a well-defined form. We introduce a DSL (Section 3) for policies (e.g. list-scheduling with priority functions, neural policy rules, or solver calls). Designing this DSL is nontrivial: it must balance expressiveness (to capture complex strategies) with simplicity (so LLMs and RL can handle it).
\item \emph{RL Reward and Stability}: We must define reward functions that capture scheduling goals (makespan, slowdown, fairness) and incorporate them into RL. Multi-objective rewards are supported via weighted sums or multi-head networks. We also need variance reduction techniques to train stably on bursty workloads, as prior work notes high variance in HPC scheduling simulations.
\item \emph{Integration with Systems}: \sys{} is designed as a user-level runtime module rather than OS kernel code. Many task-based runtimes (Legion, StarPU, OpenMP runtime) allow custom schedulers~\cite{arxiv2404}. We implement \sys{} as a plugin or user-space library. This decoupling eases deployment but means the agent controls only user-space scheduling; it must coexist with system-level CPU allocation. We assume the cluster resource allocation (nodes to jobs) is fixed by an outer scheduler (e.g. Slurm), and \sys{} optimizes job ordering, task placement, and prioritization within that job set.
\item \emph{Generalization}: The system should perform well on new workloads unseen in training. We therefore design the agent to rely on LLM generalization (zero-shot reasoning) and on meta-trained RL (e.g., training RL on a distribution of workloads). We will validate generalization on a held-out set of benchmarks.
\end{itemize}