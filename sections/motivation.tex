\section{Motivation}
\label{sec:motivation}

We motivate our work by examining this semantic gap problem and the practical safety, performance, and cost issues revealed by our experiments.

\subsection{The Semantic Gap Problem}

Linux scheduler optimization faces three fundamental barriers. First, a \textbf{domain knowledge gap} exists between developers and users: DevOps engineers lack insight into workload characteristics (latency-sensitive vs. throughput-oriented), while edge/personal device users lack both kernel optimization expertise and understanding of application-specific performance targets. Second, \textbf{technical complexity} of scheduler development requires mastering kernel programming with lock-free structures, eBPF verification constraints, and CPU/NUMA architectures, limiting innovation to few experts. Third, \textbf{dynamic workload behavior} presents complex challenges: ML training alternates between compute-intensive forward propagation and communication-heavy gradient synchronization, web traffic varies by orders of magnitude daily, and build system parallelism changes with dependencies.

Prior RL-based schedulers~\cite{mao2019decima,qiu2020firm,zhang2024mrsch,mao2019park} require extensive training per workload type, lack semantic understanding to transfer knowledge across workloads, and cannot generate new scheduling code. LLMs uniquely bridge these gaps by: (1) understanding natural language requirements and source code semantics without task-specific training, (2) synthesizing correct eBPF schedulers based on automatic workload characterization, (3) reasoning about performance trade-offs and system constraints, and critically, (4) operating in the control plane to generate optimized code that runs natively with negligible runtime overhead, unlike traditional ML models that would cause unacceptable inference latency in the scheduler hot path. This control plane separation represents a key architectural insight: LLMs generate and optimize scheduling policies offline, producing native eBPF code that executes without any ML inference overhead during actual scheduling decisions.

% \subsection{Economic Viability of AI-Driven Scheduler Optimization}

% Scheduler optimization becomes economically viable when generation costs are lower than CPU time savings. For example, with \$0.45 AI generation LLM API cost and 20\% performance improvement, a 10-hour workload on a \$2/hour instance already breaks even (\$0.45 cost vs \$4 savings). This fundamentally changes scheduler economics: traditionally, high manual engineering costs meant custom schedulers were only justified for large-scale cloud workloads running on hundreds of machines for months, but AI assistance now makes it economical to optimize even short-lived workloads like CI/CD pipelines, batch jobs, or individual builds running for just hours or days on single machines, democratizing performance optimization for previously uneconomical use cases.

\subsection{Motivation Experiment}

We tested Claude Code\cite{claudecode}, the state of the art LLM agent, with "write a FIFO scheduler in eBPF" from an empty folder, with all permissions and bash access. Of three attempts, only one succeeded. The second attempt produced pseudo-code after 6 minutes trying, and the third generated a scheduler tracer instead after 8 minutes development. The successful generation required 33 minutes, 221 LLM API calls, and 15+ iterations, costing \$6 (vs. 5 minutes typically for an expert developer). The generated code, for some workloads, exhibited poor quality with excessive overhead, performing worse than EEVDF. The agent required root access, could crash the system during testing, and lacked fallback mechanisms, which also raises safety concerns.

\subsection{Challenges in Applying LLM Agents to Schedulers}

Our experiments reveal critical challenges for AI-driven scheduler optimization, especially when fully automated: \textbf{Performance}: How do we ensure AI-generated or configured schedulers outperform existing ones rather than degrading performance? \textbf{Safety}: How do we prevent kernel crashes, soft-lockups, stalls, or starvation while maintaining stability? How can we ensure only minimal privilege needed when development and deployment? This presents a fundamental programming language challenge: synthesizing domain-specific code that must satisfy both general safety properties (memory safety, termination) and domain-specific invariants (fairness, liveness)â€”a problem that requires sophisticated verification techniques beyond standard compiler checks. \textbf{Efficiency}: The 33-minute generation time and the \$6 cost must drop for practical deployment.
