\section{Motivation}
\label{sec:motivation}

We motivate our work by examining this semantic gap problem and the practical safety, performance, and cost issues revealed by our experiments.

\subsection{The Semantic Gap Problem}

\textbf{Domain Knowledge Gap between developer and user}: In cloud environments, DevOps engineers configuring Kubernetes lack insight into workload characteristics (latency-sensitive vs. throughput-oriented), resulting in conservative scheduling. Edge and personal devices face worse challenges. Gamers, creative professionals, and office workers lack kernel expertise for optimization. LLMs can bridge this gap by understanding high-level workload patterns from source code and deployment artifacts, and translating them into concrete scheduling policies.

\textbf{Technical Complexity of Scheduler Development}: Linux scheduler development requires mastering kernel programming with lock-free structures, eBPF verification constraints, and CPU/NUMA architectures. This steep learning curve limits innovation to few kernel experts. LLMs with pre-trained domain knowledge enable rapid and automated scheduler development without human expertise.

\textbf{Dynamic Workload Adaptation}: Modern workloads exhibit complex phase behavior: ML training alternates between compute-intensive forward propagation and communication-heavy gradient synchronization; web traffic varies by orders of magnitude daily; build system parallelism changes with dependencies. Manual reconfiguration and optimization cannot keep pace, while AI agents can adapt policies in real-time, operating 24/7 without breaks, much more availability than human experts.

\subsection{Economic Viability of AI-Driven Scheduler Optimization}

Scheduler optimization becomes economically viable when generation costs are lower than CPU time savings. For example, with \$0.45 AI generation LLM API cost and 20\% performance improvement, a 10-hour workload on a \$2/hour instance already breaks even (\$0.45 cost vs \$4 savings). This fundamentally changes scheduler economics: traditionally, high manual engineering costs meant custom schedulers were only justified for large-scale cloud workloads running on hundreds of machines for months, but AI assistance now makes it economical to optimize even short-lived workloads like CI/CD pipelines, batch jobs, or individual builds running for just hours or days on single machines, democratizing performance optimization for previously uneconomical use cases.

\subsection{Motivation Experiment}

We tested Claude Code\cite{claudecode}, the state of the art LLM agent, with "write a FIFO scheduler in eBPF" from an empty folder, with all permissions and bash access. Of three attempts, only one succeeded. The second attempt produced pseudo-code after 6 minutes trying, and the third generated a scheduler tracer instead after 8 minutes development. The successful generation required 33 minutes, 221 LLM API calls, and 15+ iterations, costing \$6 (vs. 5 minutes typically for an expert developer). The generated code, for some workloads, exhibited poor quality with excessive overhead, performing worse than CFS. The agent required root access, could crash the system during testing, and lacked gradual rollout mechanisms, which also raises safety concerns.

\subsection{Challenges in Applying LLM Agents to Schedulers}

Our experiments reveal critical challenges for AI-driven scheduler optimization, especially when fully automated: \textbf{Performance}: How do we ensure AI-generated or configured schedulers outperform existing ones rather than degrading performance? \textbf{Safety}: How do we prevent kernel crashes, soft-lockups, stalls, or starvation while maintaining stability? How can we ensure only minimal privilege needed when development and deployment? \textbf{Efficiency}: The 33-minute generation time and the \$6 cost must drop for practical deployment.
