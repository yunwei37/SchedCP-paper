\section{Implementation}

We built a prototype of Sched-Agent using off-the-shelf LLMs (e.g. GPT-4/Gemini) and an RL toolkit (OpenAI Gym and Stable Baselines3). The DSL is implemented as a small Python-embedded language: users (or the agent) write policy rules in a YAML-like syntax, which are loaded into the scheduler. Our DSL supports: (1) \emph{priority rules} (conditional statements adjusting task priority), (2) \emph{resource assignments} (e.g. pin GPU-only tasks to GPUs), (3) \emph{backfill strategies} (enable/disable, with thresholds), and (4) \emph{preemption and promotion rules} for latency-sensitive tasks. The DSL is strongly typed to avoid invalid policies.

The agent-system interface follows the pattern of Wei \emph{et al.}. The LLM agent prompt includes a description of the DSL grammar and several annotated examples of policies for common cases. When invoked, the agent outputs a policy in plain text which our parser turns into an internal policy object. We experimented with \emph{ReAct-style prompting} to have the LLM produce reasoning steps along with final policy lines, which improved policy quality and interpretability.

For RL training, we extended an event-driven scheduler simulator (modeled after production systems like SLURM) to accept DSL policies as initial policies and let the agent adjust parameters. We defined the state as a summary vector of (pending queue lengths, current resource utilization, job wait times) and the actions as adjusting numeric DSL parameters or toggling rules. The reward is a weighted combination of negative makespan, average wait time, and a fairness penalty. To stabilize training on bursty trace data, we adapted the trajectory filtering technique from RLScheduler and the uncertainty-aware approach from \emph{Cilantro}, allowing the agent to explore safely.

We consider a variety of workload inputs: job traces from real HPC centers (PIK-IPLEX, SDSC [15]), synthetic DAGs (Cholesky, QR) used in READYS, streaming dataflow benchmarks, and LLM inference chains. The system is flexible enough to ingest different descriptors: e.g., a DAG file for task graphs, or a summarized cluster log for batch scheduling.

Finally, Sched-Agent can operate in online mode: after initial synthesis/tuning, it monitors actual job execution and can re-invoke the agent if patterns change (new job types appear). This enables continuous adaptation. Our prototype thus spans LLM prompting, DSL parsing, simulation, and RL, demonstrating the feasibility of the concept.