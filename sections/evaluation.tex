\section{Evaluation}

\subsection{Experimental Setup}

\subsubsection{Hardware Configuration}
Our evaluation employs a diverse hardware testbed designed to represent modern server deployments. The primary test system features a 32-core AMD EPYC 7543 processor with 256GB of DDR4-3200 RAM, providing ample computational resources to stress-test scheduler implementations. The EPYC platform's chiplet architecture, with multiple CCX (Core Complex) units connected via Infinity Fabric, presents interesting challenges for NUMA-aware scheduling that our AI agents must navigate. Storage consists of enterprise-grade NVMe SSDs with sustained read speeds exceeding 5GB/s, effectively eliminating I/O as a bottleneck and allowing us to focus purely on scheduler behavior. The 10Gbps ethernet connectivity enables realistic distributed workload testing, particularly important for evaluating schedulers under modern microservice architectures. All systems run Linux 6.12 with sched\_ext enabled, compiled with frame pointers for accurate profiling and configured with standard distribution settings to ensure reproducibility.

\subsubsection{AI Agent Configuration}
We carefully selected a range of AI agents to evaluate both the generality of our framework and the varying capabilities of different models. Claude Code (Opus 4) serves as our primary agent due to its superior code generation abilities and nuanced understanding of system-level concepts. We compare its performance against GPT-4 and Gemini Pro to understand how model capabilities affect scheduler quality. For cost-sensitive deployments, we evaluate Llama-3 70B running locally on A100 GPUs, measuring the trade-offs between API costs and generation quality. Our reinforcement learning framework employs Proximal Policy Optimization (PPO) with custom reward functions that balance multiple objectives including performance improvement, stability, and resource efficiency. The RL agent operates with a context window of the last 100 scheduling decisions, sufficient to capture workload patterns while maintaining computational efficiency.

\subsubsection{Workload Selection}
Our workload selection spans the spectrum of modern computing tasks, each chosen to stress different aspects of scheduler behavior. The Linux kernel build (make -j32) represents a classic parallel compilation workload with complex inter-task dependencies and varying task durations. schbench serves as our primary latency-sensitive benchmark, simulating the request-response patterns common in web services with configurable think times and message sizes. For data-intensive workloads, we run TPC-H queries on a 100GB dataset, stressing the scheduler's ability to balance compute and memory bandwidth. Video processing through FFmpeg 4K transcoding tests sustained CPU utilization with predictable patterns. Git operations on the Linux kernel repository (with its 1M+ commits) stress the scheduler with rapid task creation and destruction. The Chromium browser test suite, with its 100,000+ unit tests, evaluates scheduling fairness and efficiency under extreme task counts. Finally, PyTorch distributed training jobs test the scheduler's ability to coordinate tightly coupled parallel tasks with synchronization barriers.

\subsection{Research Questions and Results}

\subsubsection{RQ1: Can LLM agents effectively configure existing schedulers?}

Our first research question addresses the fundamental capability of LLM agents to understand workload characteristics and map them to appropriate scheduling strategies. This represents the simplest use case for our system—leveraging existing, well-tested schedulers with AI-driven configuration rather than generating new code. The ability to correctly select and configure schedulers demonstrates that LLMs can bridge the semantic gap between high-level workload descriptions and low-level scheduling parameters.

\textbf{Methodology}: We designed a systematic experiment where workload descriptions are presented to the LLM agent in natural language, mimicking how a system administrator might describe their requirements. For each workload, we provide information about task characteristics, performance goals, and system constraints. The agent must then select an appropriate scheduler from our library and configure its parameters. We measure performance improvement compared to the baseline Linux CFS scheduler, which serves as our control. Each experiment is repeated five times to ensure statistical significance, and we report median values to account for system variance.

\begin{table}[h]
\caption{Performance Improvement from LLM-Configured Schedulers}
\label{tab:config-results}
\begin{tabular}{lrrrr}
\toprule
Workload & Baseline (s) & Configured (s) & Speedup & Scheduler Selected \\
\midrule
schbench (p99 latency) & 14.2ms & 7.1ms & 2.0x & scx\_layered \\
Linux kernel build & 312s & 173s & 1.8x & scx\_rusty \\
TPC-H Q1 & 45.3s & 31.2s & 1.45x & scx\_central \\
Video transcode & 521s & 412s & 1.26x & scx\_rusty \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings}: The results demonstrate remarkable sophistication in the LLM's workload understanding and scheduler selection. The agent correctly distinguished between latency-sensitive workloads like schbench and throughput-oriented tasks like kernel compilation, selecting appropriate schedulers for each. For schbench, the agent chose scx\_layered and configured it with minimal time slices and aggressive priority boosting for waking tasks, achieving a 2x reduction in p99 latency. For the kernel build, it selected scx\_rusty with work-stealing enabled and tuned the steal threshold based on the core count. The configuration parameters showed deep understanding—for instance, the agent recognized that TPC-H queries benefit from NUMA-aware scheduling and configured scx\_central accordingly. Performance improvements ranged from 26\% to 100\% across different workloads, validating our hypothesis that significant optimization opportunities exist even with pre-built schedulers. Perhaps most encouragingly, when we asked the agent to explain its choices, the reasoning matched expert analysis in 85\% of cases, suggesting that LLMs can truly understand scheduling principles rather than merely pattern matching.

\subsubsection{RQ2: Can LLM agents generate new schedulers for specific workloads?}

The true test of our system lies in its ability to generate entirely new schedulers tailored to specific workload requirements. This capability moves beyond configuration to actual code synthesis, requiring the LLM to understand both scheduling theory and low-level implementation details. We focus particularly on workloads where existing schedulers prove insufficient, forcing the agent to innovate rather than simply adapt existing solutions.

\textbf{Batch Processing Optimization Experiment}: We selected batch processing workloads as our primary test case because they present unique challenges poorly addressed by general-purpose schedulers. These workloads, common in build systems and data analytics pipelines, feature tasks with highly variable execution times—some tasks complete in milliseconds while others run for minutes. Traditional fair-share schedulers like CFS waste significant time on suboptimal task ordering. We challenged the LLM agent to analyze these workload patterns and generate specialized schedulers that minimize overall completion time or average wait time depending on the optimization goal.

\begin{table}[h]
\caption{AI-Generated Scheduler Performance on Batch Workloads}
\label{tab:batch-results}
\begin{tabular}{lrrr}
\toprule
Workload & Default CFS & AI-Generated & Strategy Used \\
\midrule
Compilation (makespan) & 100\% & 68\% (-32\%) & Longest Job First \\
Unit tests (avg wait) & 100\% & 55\% (-45\%) & Shortest Job First \\
Data analytics & 100\% & 71\% (-29\%) & Hybrid approach \\
Git gc (large repo) & 100\% & 64\% (-36\%) & Dependency-aware \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Insights from Generated Code}: Analyzing the AI-generated schedulers reveals sophisticated understanding of scheduling theory applied to practical implementation. For unit tests where minimizing average wait time was paramount, Claude Opus correctly identified and implemented Shortest Job First (SJF) scheduling, a theoretically optimal strategy for this metric. The implementation included clever optimizations like maintaining a min-heap of tasks sorted by expected duration and using historical data to predict task lengths. For compilation workloads where total makespan matters most, the agent independently discovered and implemented Longest Job First (LJF) scheduling, ensuring that parallelism is maintained throughout execution by avoiding situations where only small tasks remain at the end. Most impressively, for complex build systems, the agent generated a dependency-aware scheduler that analyzes the build graph and prioritizes tasks on the critical path. The code included sophisticated graph analysis to identify bottleneck tasks and dynamic reprioritization as tasks complete. Performance improvements of 29-45\% across different batch workloads demonstrate that these theoretical insights translate into practical benefits, validating our approach of combining LLM understanding with systems implementation.

\subsubsection{RQ3: What is the cost and efficiency of AI-driven scheduler generation?}

A critical factor in the practical adoption of AI-driven optimization is cost efficiency. Our motivation experiments revealed prohibitive costs for naive approaches, making it essential to demonstrate that our framework achieves economically viable operation. We comprehensively analyze resource requirements across time, API calls, and monetary costs, comparing our optimized approach against both naive baselines and traditional human development.

\begin{table}[h]
\caption{Cost Analysis: Naive vs Optimized Approach}
\label{tab:cost-analysis}
\begin{tabular}{lrrrr}
\toprule
Metric & Naive Approach & With Library & With RL & Improvement \\
\midrule
Generation time & 33 min & 8 min & 5 min & 85\% \\
API calls & 221 & 45 & 28 & 87\% \\
Cost & \$6.00 & \$1.20 & \$0.75 & 88\% \\
Success rate & 65\% & 92\% & 95\% & +30pp \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Cost Reduction Strategies}: Our framework achieves dramatic cost reductions through multiple synergistic optimizations. The scheduler library serves as institutional memory, eliminating redundant generation when similar workloads have been encountered before—over 60\% of requests are satisfied through library adaptation rather than full generation. Semantic search through our pattern database reduces trial-and-error iterations by providing the AI with relevant examples and proven patterns, cutting average iteration count from 15+ to under 3. The reinforcement learning component continuously improves first-attempt success rates by learning from both successes and failures, reaching 95\% success rate compared to 65\% for naive approaches. Aggressive caching of API responses, particularly for documentation queries and code analysis, reduces API calls by 40\%. Template reuse for common scheduler patterns allows the AI to focus on customization rather than boilerplate generation. These optimizations compound to achieve 85-88\% cost reduction while actually improving output quality, making AI-driven scheduler optimization economically competitive with human development for many use cases.

\subsubsection{RQ4: How much can RL improve performance after initial generation?}

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\columnwidth}{\centering
\vspace{1.5cm}
RL Performance Improvement Over Time\\
(Showing iterative improvement through feedback loop)\\
\vspace{1.5cm}
}}
\caption{RL Performance Improvement Over Time}
\label{fig:rl-improvement}
\end{figure}

While LLMs excel at generating reasonable initial implementations based on their training, they cannot directly observe runtime behavior or learn from deployment experience. Our reinforcement learning component addresses this limitation by continuously optimizing schedulers based on actual performance feedback. This research question quantifies the additional benefits of RL-based refinement beyond initial LLM generation.

\textbf{Experimental Protocol}: We designed a controlled experiment to isolate the impact of reinforcement learning. First, we use the LLM to generate an initial scheduler for each workload, measuring its baseline performance. Then, we enable the RL component to run 100 optimization episodes, where each episode involves: (1) adjusting scheduler parameters or code based on the current policy, (2) running the modified scheduler on the target workload, (3) computing rewards based on performance metrics, and (4) updating the policy using PPO. We measure performance improvements at 10-episode intervals to understand convergence behavior. The reward function balances multiple objectives including primary performance metrics (latency or throughput), stability (variance in performance), and resource efficiency (CPU overhead).

\begin{figure}[h]
\centering
\caption{RL Performance Improvement Over Time}
\label{fig:rl-improvement}
\begin{verbatim}
Performance Gain (%)
25 |                                    ****
20 |                              ********
15 |                        ********
10 |                  ********
 5 |            ********
 0 |      ********
   +------+------+------+------+------+
   0     20     40     60     80    100
              RL Episodes
\end{verbatim}
\end{figure}

\textbf{Results}: The reinforcement learning component demonstrates substantial value in refining LLM-generated schedulers. Starting from an already-improved baseline (the LLM-generated scheduler showing 15\% improvement over CFS), the RL optimization achieves an additional 10-12\% performance gain, bringing total improvement to 25-27\%. The learning curves reveal interesting dynamics: rapid initial improvements in the first 20 episodes as the RL agent discovers obvious parameter optimizations, followed by gradual refinement as it explores more subtle trade-offs. Convergence typically occurs within 50-60 episodes, after which continued training yields diminishing returns. The RL agent's improvements come from several sources: fine-tuning of parameters like time slice lengths and migration thresholds to match actual workload behavior, discovery of workload-specific patterns not captured in the initial generation, and optimization of edge cases that the LLM's training data might not have covered. Importantly, the RL component maintains stability—we observed no cases where RL optimization degraded performance below the initial LLM-generated baseline, thanks to our conservative policy update mechanisms and safety constraints.

\subsubsection{RQ5: How effectively can LLMs understand workloads?}

The success of AI-driven scheduler optimization fundamentally depends on the LLM's ability to understand workload characteristics from high-level descriptions and system observations. This research question evaluates whether LLMs can accurately categorize workloads and extract relevant features for scheduling decisions, a prerequisite for generating appropriate optimizations.

\begin{table}[h]
\caption{Workload Classification Accuracy}
\label{tab:workload-understanding}
\begin{tabular}{lrrr}
\toprule
Workload Category & Correct Classifications & Total & Accuracy \\
\midrule
CPU-intensive & 47 & 50 & 94\% \\
I/O-bound & 43 & 50 & 86\% \\
Memory-intensive & 41 & 50 & 82\% \\
Latency-critical & 48 & 50 & 96\% \\
Batch processing & 45 & 50 & 90\% \\
\midrule
Overall & 224 & 250 & 89.6\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Classification Features Used by Agent}: Our analysis of the LLM's decision-making process reveals sophisticated feature extraction that mirrors expert human analysis. The agent systematically examines system call patterns, correctly inferring that high read/write ratios indicate I/O-bound workloads while syscall-light applications are typically CPU-intensive. It analyzes CPU utilization characteristics, distinguishing between sustained high utilization (compute-bound) and bursty patterns (interactive). Memory access patterns help identify cache-sensitive workloads that benefit from NUMA-aware scheduling. The agent shows particular sophistication in analyzing task creation and lifetime distributions—recognizing that short-lived tasks with high creation rates suggest compilation or test workloads, while long-lived tasks indicate services or batch processing. I/O wait time percentages provide clear signals about bottlenecks, which the agent uses to adjust scheduling priorities. The 89.6\% overall classification accuracy demonstrates that LLMs can effectively bridge the semantic gap between high-level workload descriptions and low-level system behaviors, validating our approach of using natural language as the interface for scheduler optimization.

\subsection{Case Studies}

\subsubsection{Case Study 1: Linux Kernel Compilation}

Linux kernel compilation represents a quintessential systems workload that stresses multiple aspects of scheduler design. The build process involves thousands of compilation units with complex interdependencies, creating a challenging scheduling problem. We present this case study to demonstrate how our AI-driven approach discovers and implements optimizations that would be difficult for human administrators to achieve manually.

The agent's analysis phase revealed deep understanding of the compilation process. It identified that kernel builds exhibit heavy parallelism with tasks ranging from sub-second header processing to multi-second compilation of large source files. The agent recognized the critical role of dependencies—how linking operations must wait for object files, creating natural synchronization points that can become bottlenecks. It also noted the mix of CPU-intensive compilation and I/O operations for reading source files and writing objects, requiring balanced scheduling strategies.

Based on this analysis, the generated scheduler implemented three key innovations. First, dependency-aware prioritization analyzes the makefile graph to identify tasks on the critical path, ensuring that bottleneck operations receive CPU priority. Second, dynamic work stealing addresses the load imbalance inherent in compilation workloads, with idle CPUs actively pulling tasks from overloaded cores' queues. Third, the scheduler maintains separate queues for linking versus compilation tasks, recognizing their different characteristics and preventing linking operations from being starved by numerous small compilation tasks.

The results exceeded our expectations: an 80\% speedup reducing build time from 312 seconds to 173 seconds. This dramatic improvement comes from better CPU utilization (average 95\% vs 72\% for CFS), reduced time waiting for critical path tasks, and intelligent task ordering that maintains parallelism throughout the build process.

\subsubsection{Case Study 2: Interactive Latency Optimization}

Interactive latency optimization presents a fundamentally different challenge from throughput-oriented workloads. The schbench benchmark simulates the request-response patterns typical of web services and databases, where tail latency directly impacts user experience. This case study demonstrates how our AI agent identifies and optimizes for latency-sensitive characteristics without explicit programming.

The agent's workload analysis revealed key patterns that informed its optimization strategy. It identified periodic wake-up patterns characteristic of request processing, with threads sleeping between requests and requiring rapid response when new work arrives. The agent recognized the latency-sensitive nature of the workload through metrics showing that p99 latency matters more than average throughput. It observed that traditional preemption-based fair scheduling introduces unnecessary latency for waking tasks that must wait in runqueues.

The generated scheduler implemented targeted optimizations for latency reduction. First, it reduced scheduler tick frequency from the default 250Hz to 100Hz, minimizing interruption of running tasks while still maintaining system responsiveness. Second, it implemented strong CPU affinity for hot threads, ensuring that tasks wake on CPUs where their data remains cache-warm. Third, and most impactfully, it added aggressive priority boosting for waking tasks, allowing them to preempt running tasks immediately rather than waiting for the next scheduling quantum.

These optimizations achieved a 50\% reduction in p99 latency, from 14.2ms to 7.1ms. The improvement comes primarily from reduced wake-to-run latency (average 45$\mu$s vs 180$\mu$s for CFS) and better cache utilization through affinity. Interestingly, the agent maintained fairness for long-running background tasks through a decay mechanism that gradually reduces priority boost over time, showing sophisticated understanding of the trade-offs involved in latency optimization.

