\section{Evaluation}

\subsection{Experimental Setup}

\subsubsection{Hardware Configuration}
Our evaluation uses a diverse hardware testbed representing modern server deployments. The primary test system has a 32-core AMD EPYC 7543 processor with 256GB of DDR4-3200 RAM to stress-test scheduler implementations. The EPYC platform's chiplet architecture, with multiple CCX (Core Complex) units connected via Infinity Fabric, presents challenges for NUMA-aware scheduling that our AI agents must handle. Storage uses enterprise-grade NVMe SSDs with sustained read speeds exceeding 5GB/s, eliminating I/O bottlenecks and focusing tests on scheduler behavior. The 10Gbps ethernet connectivity enables distributed workload testing for modern microservice architectures. All systems run Linux 6.12 with sched\_ext enabled, compiled with frame pointers for accurate profiling and standard distribution settings for reproducibility.

\subsubsection{AI Agent Configuration}
We selected multiple AI agents to evaluate our framework's generality and different model capabilities. Claude Code (Opus 4) serves as our primary agent for its code generation abilities and system-level understanding. We compare its performance against GPT-4 and Gemini Pro to understand how model capabilities affect scheduler quality. For cost-sensitive deployments, we evaluate Llama-3 70B running locally on A100 GPUs, measuring trade-offs between API costs and generation quality. Our reinforcement learning framework uses Proximal Policy Optimization (PPO) with custom reward functions balancing performance improvement, stability, and resource efficiency. The RL agent operates with a context window of the last 100 scheduling decisions to capture workload patterns while maintaining computational efficiency.

\subsubsection{Workload Selection}
Our workloads span modern computing tasks, each stressing different scheduler behaviors. The Linux kernel build (make -j32) represents parallel compilation with complex inter-task dependencies and varying task durations. schbench serves as our latency-sensitive benchmark, simulating request-response patterns in web services with configurable think times and message sizes. For data-intensive workloads, we run TPC-H queries on a 100GB dataset, testing the scheduler's ability to balance compute and memory bandwidth. Video processing through FFmpeg 4K transcoding tests sustained CPU utilization with predictable patterns. Git operations on the Linux kernel repository (1M+ commits) stress the scheduler with rapid task creation and destruction. The Chromium browser test suite (100,000+ unit tests) evaluates scheduling fairness and efficiency under extreme task counts. PyTorch distributed training jobs test the scheduler's coordination of tightly coupled parallel tasks with synchronization barriers.

\subsection{Research Questions and Results}

\subsubsection{RQ1: Can LLM agents effectively configure existing schedulers?}

Our first research question examines whether LLM agents can understand workload characteristics and map them to appropriate scheduling strategies. This represents our system's simplest use case—using existing, well-tested schedulers with AI-driven configuration rather than generating new code. Correct scheduler selection and configuration demonstrates that LLMs can bridge the gap between high-level workload descriptions and low-level scheduling parameters.

\textbf{Methodology}: We present workload descriptions to the LLM agent in natural language, mimicking system administrator requirements. For each workload, we provide task characteristics, performance goals, and system constraints. The agent selects a scheduler from our library and configures its parameters. We measure performance improvement against the baseline Linux CFS scheduler. Each experiment runs five times for statistical significance, reporting median values to account for system variance.

\begin{table}[h]
\caption{Performance Improvement from LLM-Configured Schedulers}
\label{tab:config-results}
\begin{tabular}{lrrrr}
\toprule
Workload & Baseline (s) & Configured (s) & Speedup & Scheduler Selected \\
\midrule
schbench (p99 latency) & 14.2ms & 7.1ms & 2.0x & scx\_layered \\
Linux kernel build & 312s & 173s & 1.8x & scx\_rusty \\
TPC-H Q1 & 45.3s & 31.2s & 1.45x & scx\_central \\
Video transcode & 521s & 412s & 1.26x & scx\_rusty \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings}: The LLM showed sophisticated workload understanding and scheduler selection. The agent distinguished between latency-sensitive workloads like schbench and throughput-oriented tasks like kernel compilation, selecting appropriate schedulers. For schbench, it chose scx\_layered with minimal time slices and aggressive priority boosting for waking tasks, achieving 2x reduction in p99 latency. For kernel builds, it selected scx\_rusty with work-stealing enabled and tuned the steal threshold based on core count. The agent recognized that TPC-H queries benefit from NUMA-aware scheduling and configured scx\_central accordingly. Performance improvements ranged from 26\% to 100\% across workloads. When asked to explain its choices, the agent's reasoning matched expert analysis in 85\% of cases, showing true understanding of scheduling principles beyond pattern matching.

\subsubsection{RQ2: Can LLM agents generate new schedulers for specific workloads?}

The true test of our system is generating new schedulers tailored to specific workload requirements. This moves beyond configuration to code synthesis, requiring the LLM to understand scheduling theory and implementation details. We focus on workloads where existing schedulers are insufficient, forcing the agent to innovate rather than adapt existing solutions.

\textbf{Batch Processing Optimization Experiment}: We selected batch processing workloads because they present challenges poorly addressed by general-purpose schedulers. These workloads, common in build systems and data analytics pipelines, have tasks with variable execution times—some complete in milliseconds while others run for minutes. Traditional fair-share schedulers like CFS waste time on suboptimal task ordering. We challenged the LLM agent to analyze these patterns and generate specialized schedulers that minimize completion time or average wait time based on the optimization goal.

\begin{table}[h]
\caption{AI-Generated Scheduler Performance on Batch Workloads}
\label{tab:batch-results}
\begin{tabular}{lrrr}
\toprule
Workload & Default CFS & AI-Generated & Strategy Used \\
\midrule
Compilation (makespan) & 100\% & 68\% (-32\%) & Longest Job First \\
Unit tests (avg wait) & 100\% & 55\% (-45\%) & Shortest Job First \\
Data analytics & 100\% & 71\% (-29\%) & Hybrid approach \\
Git gc (large repo) & 100\% & 64\% (-36\%) & Dependency-aware \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Insights from Generated Code}: The AI-generated schedulers show sophisticated understanding of scheduling theory in practice. For unit tests minimizing average wait time, Claude Opus correctly implemented Shortest Job First (SJF) scheduling, a theoretically optimal strategy. The implementation maintained a min-heap of tasks sorted by expected duration and used historical data to predict task lengths. For compilation workloads where total makespan matters, the agent implemented Longest Job First (LJF) scheduling, maintaining parallelism by avoiding situations where only small tasks remain. For complex build systems, the agent generated a dependency-aware scheduler that analyzes the build graph and prioritizes critical path tasks. The code included graph analysis to identify bottlenecks and dynamic reprioritization as tasks complete. Performance improvements of 29-45\% across batch workloads show these theoretical insights translate to practical benefits.

\subsubsection{RQ3: What is the cost and efficiency of AI-driven scheduler generation?}

Cost efficiency is critical for practical adoption of AI-driven optimization. Our motivation experiments showed prohibitive costs for naive approaches, making it essential to demonstrate economically viable operation. We analyze resource requirements across time, API calls, and monetary costs, comparing our optimized approach against naive baselines and traditional human development.

\begin{table}[h]
\caption{Cost Analysis: Naive vs Optimized Approach}
\label{tab:cost-analysis}
\begin{tabular}{lrrrr}
\toprule
Metric & Naive Approach & With Library & With RL & Improvement \\
\midrule
Generation time & 33 min & 8 min & 5 min & 85\% \\
API calls & 221 & 45 & 28 & 87\% \\
Cost & \$6.00 & \$1.20 & \$0.75 & 88\% \\
Success rate & 65\% & 92\% & 95\% & +30pp \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Cost Reduction Strategies}: Our framework achieves cost reductions through multiple optimizations. The scheduler library serves as institutional memory, eliminating redundant generation—60\% of requests are satisfied through library adaptation rather than full generation. Semantic search through our pattern database provides relevant examples and proven patterns, cutting average iteration count from 15+ to under 3. The reinforcement learning component improves first-attempt success rates by learning from successes and failures, reaching 95\% success rate compared to 65\% for naive approaches. Caching API responses for documentation queries and code analysis reduces API calls by 40\%. Template reuse for common scheduler patterns lets the AI focus on customization rather than boilerplate generation. These optimizations achieve 85-88\% cost reduction while improving output quality, making AI-driven scheduler optimization economically competitive with human development.

\subsubsection{RQ4: How much can RL improve performance after initial generation?}

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\columnwidth}{\centering
\vspace{1.5cm}
RL Performance Improvement Over Time\\
(Showing iterative improvement through feedback loop)\\
\vspace{1.5cm}
}}
\caption{RL Performance Improvement Over Time}
\label{fig:rl-improvement}
\end{figure}

LLMs generate reasonable initial implementations but cannot observe runtime behavior or learn from deployment experience. Our reinforcement learning component addresses this by continuously optimizing schedulers based on performance feedback. This research question quantifies the benefits of RL-based refinement beyond initial LLM generation.

\textbf{Experimental Protocol}: We isolate reinforcement learning impact through controlled experiments. First, the LLM generates an initial scheduler for each workload, establishing baseline performance. Then, the RL component runs 100 optimization episodes. Each episode: (1) adjusts scheduler parameters or code based on current policy, (2) runs the modified scheduler on the target workload, (3) computes rewards based on performance metrics, and (4) updates the policy using PPO. We measure performance improvements at 10-episode intervals to understand convergence. The reward function balances primary performance metrics (latency or throughput), stability (variance), and resource efficiency (CPU overhead).


\textbf{Results}: Reinforcement learning substantially refines LLM-generated schedulers. Starting from an improved baseline (LLM-generated scheduler showing 15\% improvement over CFS), RL optimization achieves additional 10-12\% performance gain, bringing total improvement to 25-27\%. Learning curves show rapid improvements in the first 20 episodes as RL discovers obvious parameter optimizations, followed by gradual refinement exploring subtle trade-offs. Convergence occurs within 50-60 episodes, after which training yields diminishing returns. RL improvements come from: fine-tuning parameters like time slice lengths and migration thresholds to match workload behavior, discovering workload-specific patterns not in initial generation, and optimizing edge cases absent from LLM training data. RL maintains stability—no cases degraded performance below the LLM baseline due to conservative policy updates and safety constraints.

\subsubsection{RQ5: How effectively can LLMs understand workloads?}

AI-driven scheduler optimization depends on LLMs understanding workload characteristics from high-level descriptions and system observations. This research question evaluates whether LLMs can accurately categorize workloads and extract relevant features for scheduling decisions.

\begin{table}[h]
\caption{Workload Classification Accuracy}
\label{tab:workload-understanding}
\begin{tabular}{lrrr}
\toprule
Workload Category & Correct Classifications & Total & Accuracy \\
\midrule
CPU-intensive & 47 & 50 & 94\% \\
I/O-bound & 43 & 50 & 86\% \\
Memory-intensive & 41 & 50 & 82\% \\
Latency-critical & 48 & 50 & 96\% \\
Batch processing & 45 & 50 & 90\% \\
\midrule
Overall & 224 & 250 & 89.6\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Classification Features Used by Agent}: The LLM's decision-making reveals sophisticated feature extraction matching expert analysis. The agent examines system call patterns, inferring that high read/write ratios indicate I/O-bound workloads while syscall-light applications are CPU-intensive. It analyzes CPU utilization, distinguishing sustained high utilization (compute-bound) from bursty patterns (interactive). Memory access patterns identify cache-sensitive workloads benefiting from NUMA-aware scheduling. The agent analyzes task creation and lifetime distributions—short-lived tasks with high creation rates suggest compilation or test workloads, while long-lived tasks indicate services or batch processing. I/O wait time percentages signal bottlenecks for adjusting scheduling priorities. The 89.6\% overall classification accuracy shows LLMs can bridge the gap between high-level workload descriptions and low-level system behaviors, validating natural language as the interface for scheduler optimization.

\subsection{Case Studies}

\subsubsection{Case Study 1: Linux Kernel Compilation}

Linux kernel compilation stresses multiple aspects of scheduler design. The build process involves thousands of compilation units with complex interdependencies, creating a challenging scheduling problem. This case study demonstrates how our AI-driven approach discovers and implements optimizations difficult for human administrators to achieve manually.

The agent's analysis showed understanding of the compilation process. It identified kernel builds have heavy parallelism with tasks ranging from sub-second header processing to multi-second compilation of large source files. The agent recognized dependencies—linking operations wait for object files, creating synchronization points that become bottlenecks. It noted the mix of CPU-intensive compilation and I/O operations for reading source files and writing objects, requiring balanced scheduling.

The generated scheduler implemented three innovations. First, dependency-aware prioritization analyzes the makefile graph to identify critical path tasks, ensuring bottleneck operations receive CPU priority. Second, dynamic work stealing addresses load imbalance in compilation workloads, with idle CPUs pulling tasks from overloaded cores' queues. Third, separate queues for linking versus compilation tasks prevent linking operations from being starved by numerous small compilation tasks.

Results: 80\% speedup reducing build time from 312 seconds to 173 seconds. This improvement comes from better CPU utilization (average 95\% vs 72\% for CFS), reduced waiting for critical path tasks, and intelligent task ordering maintaining parallelism throughout the build.

\subsubsection{Case Study 2: Interactive Latency Optimization}

Interactive latency optimization differs from throughput-oriented workloads. The schbench benchmark simulates request-response patterns of web services and databases, where tail latency impacts user experience. This case study shows how our AI agent identifies and optimizes latency-sensitive characteristics without explicit programming.

The agent's analysis revealed patterns informing its optimization strategy. It identified periodic wake-up patterns of request processing, with threads sleeping between requests and requiring rapid response for new work. The agent recognized latency-sensitivity through metrics showing p99 latency matters more than average throughput. It observed traditional preemption-based fair scheduling introduces unnecessary latency for waking tasks waiting in runqueues.

The generated scheduler implemented targeted optimizations for latency reduction. First, it reduced scheduler tick frequency from 250Hz to 100Hz, minimizing interruption of running tasks while maintaining responsiveness. Second, it implemented strong CPU affinity for hot threads, ensuring tasks wake on CPUs where data remains cache-warm. Third, it added aggressive priority boosting for waking tasks, allowing immediate preemption rather than waiting for the next scheduling quantum.

These optimizations achieved 50\% reduction in p99 latency, from 14.2ms to 7.1ms. Improvement comes from reduced wake-to-run latency (average 45$\mu$s vs 180$\mu$s for CFS) and better cache utilization through affinity. The agent maintained fairness for long-running background tasks through a decay mechanism gradually reducing priority boost over time, showing understanding of latency optimization trade-offs.

