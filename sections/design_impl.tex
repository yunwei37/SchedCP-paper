\section{The \sys\ Framework: System Design and Implementation}
\label{sec:schedcp_framework}

Our approach to agentic OS optimization is founded on a clean separation between the systems infrastructure and the AI logic. \textbf{\sys} is the systems framework, a stable and secure control plane that acts as an "API for OS optimization." It provides the essential tools and safety guarantees necessary for any AI agent to interact with the Linux kernel's scheduler without compromising stability. This section details its design principles, core components, and implementation.
We are building a system interface that can be used by AI agents. Our core insight is that AI agents are fundamentally context engineering systems. They need sufficient information to make decisions but not so much that costs become prohibitive. This mirrors human experts performing optimization: they need the right tools to collect profiling data and implement policies with appropriate frameworks. As system researchers, our goal is not to design better AI agents, but to design better systems and interfaces that can be used by AI agents. This is similar to provide a framework for reinforcement learning enviroments, where the agents can learn from it.

\subsection{Design Principles}
The design of \sys\ is governed by four key principles that ensure it is safe, efficient, and future-proof.

\textbf{Decoupling and Role Separation}: A system tightly coupled to a specific AI model's capabilities will quickly become obsolete as models evolve. To ensure our framework is future-proof, we believe the system's role must be separated from the AI's. Our principle is to decouple ``what to optimize'' (the AI's domain) from ``how to observe and act'' (the system's domain). We treat the AI agent as a performance engineer using a stable set of tools provided by the system, allowing the framework to remain relevant even as AI capabilities advance.

\textbf{Safety-First Interface Design}: Giving an autonomous agent direct control over a kernel component is inherently risky, as it can generate incorrect or even malicious code. Because system stability is non-negotiable, we cannot simply trust the agent to be cautious. Therefore, our core principle is to treat the AI as a potentially non-cautious actor and design defensive interfaces. The system must insulate itself from flawed or malicious agent-generated code by default, preventing catastrophic failures rather than relying on the agent to avoid them.

\textbf{Context and Feedback Balance}: LLM agents are constrained by finite context windows and token costs, and their performance can degrade when flooded with irrelevant data, where the ``signal is lost in the noise.'' This creates a fundamental tension between providing complete information and maintaining model focus. Our core principle is therefore adaptive context provisioning. We believe the framework must enable the agent to start with a minimal summary and progressively request more detailed information as needed, creating a crucial trade-off between cost and precision.

\textbf{Composable Tool Architecture}: A rigid, prescriptive workflow would stifle an LLM's greatest strength: its ability to reason and devise novel solutions dynamically. To leverage this unique capability, we believe the framework must not be prescriptive. Following the original Unix philosophy, our principle is to provide a set of powerful, atomic tools and leave the complex workflow construction and planning to the agent's reasoning capabilities, allowing it to generate novel solutions.

\subsection{Core Components and Implementation}
SchedCP is engineered as a modular control plane, exposing its services to AI agents via the standard Model Context Protocol (MCP)~\cite{anthropic2024mcp}. This design cleanly separates the high-level policy orchestration managed by the agent from the low-level observation and execution handled by the framework, and avoids granting `root' privileges to the agent. The architecture consists of three primary services.

\textbf{1. Workload Analysis Engine.} This service acts as the agent's sensorium, providing a multi-layered, adaptive view of the system state to balance context richness against token cost. It offers cost-effective API endpoints for pre-processed, high-level summaries of system state, such as CPU load and memory usage. For deeper investigation, it provides secure, sandboxed access to standard Linux profiling tools like \texttt{perf} and \texttt{strace}, as well as a library of dynamically attachable eBPF-based probes for collecting fine-grained, workload-specific data with low overhead. Crucially, this service also closes the short-term learning loop by providing a feedback channel that allows the agent to observe the performance outcomes of its deployed policies. This mechanism delivers a concise reward signal, such as percentage change in makespan or latency, which is essential for the agent to perform in-context reinforcement learning and iterative refinement.

\textbf{2. Scheduler Policy Repository.} To reduce the high cost and latency of generating policies from scratch, the Scheduler Policy Repository serves as a persistent, evolving library of scheduling solutions. It is implemented using a vector database that stores eBPF scheduler code alongside rich metadata, including natural language descriptions of their purpose, target workloads, and historical performance metrics. This enables the agent to perform efficient semantic searches to retrieve relevant schedulers or composable code primitives from which to construct new policies. To ensure the system improves over time, this service is responsible for long-term knowledge curation. It asynchronously processes structured performance data from deployments to update the historical metrics and metadata associated with each scheduler. Highly successful, novel policies generated by the agent can thus be vetted and promoted into the permanent library, enriching the system's collective intelligence and operational memory.

\textbf{3. Execution Verifier.} This service is the cornerstone of SchedCP's safety-first design, providing a non-negotiable, multi-stage validation pipeline that all agent-generated code must pass before deployment. An agent submits a code artifact to the verifier, which first subjects it to a static analysis sandbox for eBPF verifier simulation and complexity analysis to preemptively catch safety violations and estimate overhead. Code that passes is then compiled and executed within a dynamic validation sandbox, typically a lightweight micro-VM, to undergo a battery of correctness and performance regression tests against synthetic workloads. Only upon passing this rigorous vetting does the service issue a signed deployment token to the agent. The subsequent canary deployment is overseen by a circuit breaker mechanism that continuously monitors key performance indicators and automatically reverts to the last known-good scheduler if predefined safety thresholds are breached, ensuring system stability is never compromised.

\section{\agent: A Multi-Agent Framework for OS Optimization}
\label{sec:sched_agents}

Building on the \sys\ foundation, we developed \textbf{\agent}, a multi-agent AI framework where specialized agents collaborate to perform the end-to-end task of scheduler optimization. We implemented this framework using Claude Code's subagent architecture~\cite{anthropic2024subagents}, which enables specialized AI assistants to handle specific tasks with customized system prompts, tools, and separate context windows~\cite{anthropic2024multiagent}. This approach decomposes the complex reasoning process into manageable roles, mirroring an expert human team. Each agent corresponds to one of the four stages in our optimization loop: observation, planning, execution, and learning.

\subsection{Agent Roles and Responsibilities}
The \agent\ framework is composed of four distinct agents, each with a specialized role in the optimization loop, corresponding to the four stages of our control loop: Observation, Planning, Execution, and Learning.

\subsubsection{Observation \& Analysis Agent - Building a Workload Profile}

The \textbf{Observation Agent} orchestrates workload analysis by intelligently leveraging the Workload Analysis Engine to synthesize a structured ``Workload Profile.'' Working within the secure Analysis Sandbox provided by the Engine, the agent strategically sequences diagnostic operations to build a comprehensive understanding of the workload.

The agent begins by examining high-level artifacts like source code and Makefiles, then progressively deepens its investigation through targeted profiling. It dynamically composes tool sequences based on initial findings, for instance, after discovering parallel compilation patterns, it might run \texttt{perf stat} to quantify CPU utilization and \texttt{strace} to trace inter-process dependencies. The agent adapts its analysis strategy in real-time, creating custom analysis scripts when standard tools prove insufficient. Throughout this process, the agent balances context richness against token cost by pulling only the information it needs. The agent transforms raw observations into a Workload Profile containing natural language descriptions of the workload's purpose, quantified performance characteristics, and explicit optimization goals. It also registers for event notifications, enabling reactive re-analysis when workload behavior changes.

\subsubsection{Planning Agent - Policy Synthesis and Selection}

The \textbf{Planning Agent} transforms the Workload Profile into an actionable optimization strategy by intelligently querying the Scheduler Policy Repository. Using the profile's semantic descriptions and performance goals, the agent constructs targeted repository queries to identify relevant scheduling solutions.

The agent employs a hierarchical decision process: it searches for exact matches using workload keywords, then broadens to similar workload patterns if needed. Based on query results and their associated performance metrics, the agent selects one of three optimization pathways. For perfect matches, it extracts configuration parameters from successful deployments. For partial matches, it retrieves source code and synthesizes patches that adapt the scheduler to the specific workload requirements. When no suitable base exists, the agent composes a new scheduler by selecting and combining algorithm primitives from the repository. Throughout this process, the agent reasons about trade-offs between reuse efficiency and customization benefits, leveraging the repository's historical performance data to inform its decisions.

\subsubsection{Execution Agent - Validated Policy Deployment}

The \textbf{Execution Agent} shepherds scheduling policies through the Execution Verifier, orchestrating the validation and deployment process. Taking the optimization plan from the Planning Agent, it first synthesizes deployable code artifacts - either patches for existing schedulers or complete eBPF programs.

The agent manages a multi-phase interaction with the Execution Verifier. It submits code artifacts and interprets validation results at each stage, adapting its approach based on feedback. When static analysis reveals safety violations or performance concerns, the agent iteratively refines the code. Upon passing static checks, it monitors dynamic validation results, analyzing test failures to identify and fix logical errors. The agent maintains responsibility throughout this process, deciding when to proceed, retry with modifications, or abandon an approach entirely. Once validation succeeds and the verifier issues a deployment token, the agent initiates the canary rollout and monitors initial performance metrics. If the circuit breaker triggers during deployment, the agent captures the failure context and determines whether to attempt a revised approach or escalate to the Learning Agent for deeper analysis.

\subsubsection{Learning Agent - Performance Analysis and Knowledge Update}

The \textbf{Learning Agent} closes the optimization loop by analyzing deployment outcomes and updating the system's collective knowledge. Upon receiving performance metrics through the Feedback Channel, the agent performs both immediate analysis and long-term knowledge curation.

For immediate learning, the agent retrieves and interprets performance summaries, identifying success patterns and failure modes to inform subsequent optimization cycles. The agent then curates this knowledge for long-term benefit: it updates the Scheduler Policy Repository with refined performance metrics, annotates schedulers with deployment contexts and outcomes, and promotes highly successful novel policies into the permanent library. When deployments fail or underperform, the agent documents antipatterns and constraints to prevent future agents from repeating mistakes. This dual approach ensures both rapid adaptation within the current optimization session and persistent system-wide learning across deployments.


\subsection{Example: Kernel Compilation}

To illustrate how these four agents work together, consider a kernel compilation workload. The \textbf{Observation Agent} begins by analyzing the Linux kernel source tree, executing \texttt{make -j} to understand the build process, and running \texttt{perf stat} to profile resource usage. This observation produces a Workload Profile: ``CPU-intensive parallel compilation task with short-lived processes, inter-process dependencies, and a goal to minimize makespan.'' During planning, the \textbf{Planning Agent} queries the Scheduler Policy Repository with keywords like ``throughput'' and ``compilation,'' retrieving \texttt{scx\_rusty} as a starting point. It generates a patch to make the scheduler dependency-aware. In execution, the \textbf{Execution Agent} submits the patched code to the Execution Verifier for validation, receiving a deployment token upon success. Finally, after deployment, the \textbf{Learning Agent} receives feedback that the revision achieved a 45\% reduction in makespan, contributing the improved scheduler back to the Scheduler Policy Repository for future use. This entire workflow demonstrates how \agent\ enables AI agents to autonomously optimize system performance through iterative refinement.


