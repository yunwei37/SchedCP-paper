\section{System Design}

\subsection{Design Philosophy and Constraints}

Our system is built on the understanding that AI agents are fundamentally context engineering systemsâ€”they need sufficient information to make decisions but not so much that costs become prohibitive. This mirrors how human experts approach optimization: using the right tools to collect profiling data and implementing programmable policies with appropriate frameworks. As system researchers, our goal is not to design better AI agents, but to design better systems and interfaces that can be effectively used by AI agents.

\subsubsection{Decoupling and Role Separation}
\textbf{Principle}: Separate ``what to optimize'' (AI problem) from ``how to observe and act'' (system problem).

We treat AI agents as performance engineers, establishing clear interface boundaries:
\begin{itemize}
\item \textbf{AI Responsibilities}: Understand workload patterns, identify optimization opportunities, generate scheduling strategies
\item \textbf{System Responsibilities}: Provide observations, execute actions safely, enforce constraints
\item \textbf{Evolution-Ready Design}: Future AI models can leverage the same interfaces without system redesign
\end{itemize}

This separation ensures that as AI capabilities improve, the system naturally benefits without architectural changes.

\subsubsection{Context and Feedback Balance}
\textbf{Principle}: Provide sufficient context for good decisions while controlling costs.

AI agents face a fundamental trade-off between information completeness and token costs:
\begin{itemize}
\item \textbf{Adaptive Context Window}: Start with minimal context for simple decisions, progressively add detail for complex scenarios
\item \textbf{Learned Relevance}: Track which context matters for which workload types
\item \textbf{Feedback Integration}: Add previous results to context for improved future decisions
\item \textbf{Cost-Aware APIs}: Expose controls for information granularity based on decision importance
\end{itemize}

\subsubsection{Composable Tool Architecture}
\textbf{Principle}: Leverage LLM code generation abilities through programmable, composable tools.

Following Unix philosophy, we provide:
\begin{itemize}
\item \textbf{Atomic Tools}: Basic operations like \texttt{get\_cpu\_stats()}, \texttt{set\_scheduler\_param()}
\item \textbf{Compositional Tools}: Complex workflows built from atomic operations
\item \textbf{Dynamic Tool Chains}: AI can create new tool combinations for novel scenarios
\item \textbf{Generated Tools}: AI writes custom analysis scripts on demand
\end{itemize}

Example tool composition:
\begin{verbatim}
# Atomic tools
stats = get_cpu_stats()
set_scheduler_param("priority_boost", 5)

# Composed tool
def profile_workload():
    stats = []
    for phase in workload_phases:
        stats.append(get_cpu_stats())
        analyze_pattern(stats)
    return optimization_strategy(stats)

# AI-generated tool
def custom_analysis():
    # AI writes this based on specific needs
    if detect_compilation_pattern():
        prioritize_by_dependency_graph()
\end{verbatim}

\subsubsection{Safety-First Interface Design}
\textbf{Core Principle}: Treat AI as a potentially non-cautious actor requiring defensive interfaces.

\textbf{Defensive API Design}:
\begin{itemize}
\item No single API call can crash the system
\item All operations have built-in bounds and limits
\item Example: \texttt{set\_cpu\_limit()} has hard max of 95\% to prevent starvation
\end{itemize}

\textbf{Staged Execution with Validation}:
\begin{itemize}
\item Preview mode shows effects before execution
\item Dry-run capabilities for all destructive operations
\item Mandatory validation between generation and deployment
\end{itemize}

\textbf{Constrained Action Space}:
\begin{itemize}
\item Whitelist allowed operations, not blacklist dangerous ones
\item Graduated permissions based on success history
\item New agents limited to 20\% parameter variations initially
\end{itemize}

\textbf{Automatic Safety Rails}:
\begin{itemize}
\item System-enforced resource limits
\item Circuit breakers for performance degradation
\item Watchdog timers for all AI operations
\item Clear audit trails for debugging
\end{itemize}

\subsection{Core System Components}

\subsubsection{Multi-Layer RL LLM Agent}
Our agent architecture consists of three specialized layers working in concert:

\textbf{Decision Layer}:
\begin{itemize}
\item Analyzes workload descriptions and system state
\item Selects high-level strategy: configure existing, modify, or create new scheduler
\item Reasons about trade-offs between different approaches
\end{itemize}

\textbf{Implementation Layer}:
\begin{itemize}
\item Translates decisions into concrete code or configurations
\item Generates eBPF C code and Rust userspace components
\item Ensures code follows sched\_ext conventions and safety requirements
\end{itemize}

\textbf{Learning Layer}:
\begin{itemize}
\item Applies reinforcement learning from performance feedback
\item Updates strategy selection based on outcomes
\item Maintains context memory for future decisions
\end{itemize}

\subsubsection{AI-Managed Scheduler Library}
The scheduler library serves as institutional memory and reusable component repository:

\textbf{Library Entry Structure}:
\begin{itemize}
\item \textbf{Description}: Natural language summary of purpose and characteristics
\item \textbf{Configuration Parameters}: Well-defined tunables with ranges and defaults
\item \textbf{Source Code}: Complete eBPF and userspace implementation
\item \textbf{Performance History}: Metrics from previous deployments
\item \textbf{Test Results}: Verification status and benchmark scores
\end{itemize}

\textbf{Example Entry}:
\begin{verbatim}
{
  "name": "scx_compilation_aware",
  "description": "Optimizes for software builds by prioritizing 
                  tasks based on dependency depth",
  "parameters": {
    "boost_factor": {"default": 2.0, "range": [1.0, 5.0]},
    "dep_threshold": {"default": 3, "range": [1, 10]}
  },
  "performance": {
    "kernel_build": {"speedup": 1.8, "samples": 50},
    "make_world": {"speedup": 1.5, "samples": 20}
  },
  "source": "path/to/scheduler.bpf.c"
}
\end{verbatim}

\subsubsection{Static Analysis \& Testing Framework}
Safety-critical validation before any scheduler deployment:

\textbf{BPF Verification Pipeline}:
\begin{itemize}
\item Pre-validate against kernel verifier rules
\item Check for common pitfalls (infinite loops, invalid memory access)
\item Estimate verification complexity to avoid timeouts
\end{itemize}

\textbf{Performance Estimation}:
\begin{itemize}
\item Static analysis of scheduler overhead
\item Instruction count and branch complexity metrics
\item Cache behavior prediction
\end{itemize}

\textbf{Automated Testing}:
\begin{itemize}
\item Unit tests for individual scheduler functions
\item Integration tests with simulated workloads
\item Stress tests for edge cases and error conditions
\end{itemize}

\subsubsection{Scheduler Optimization Libraries}
Pre-built patterns and primitives for rapid scheduler development:

\textbf{Production Scheduler Collection}:
\begin{itemize}
\item \texttt{scx\_rusty}: Work-stealing scheduler with load balancing
\item \texttt{scx\_layered}: Hierarchical scheduling with cgroup awareness
\item \texttt{scx\_central}: Centralized queue scheduler for NUMA systems
\end{itemize}

\textbf{Algorithm Primitives}:
\begin{itemize}
\item FIFO, LIFO, priority queue implementations
\item CFS-style virtual runtime tracking
\item Work-stealing and migration policies
\item CPU affinity and NUMA awareness patterns
\end{itemize}

\textbf{Smart Retrieval}:
\begin{itemize}
\item Semantic search using workload descriptions
\item Performance-based ranking from historical data
\item Automatic pattern extraction from successful schedulers
\end{itemize}

\subsubsection{Profiling \& Monitoring Toolkit}
Comprehensive workload analysis and system observability:

\textbf{Workload Profiling}:
\begin{itemize}
\item CPU usage patterns and phase detection
\item Memory access patterns and working set sizes
\item I/O behavior and syscall frequencies
\item Task creation rates and lifetimes
\end{itemize}

\textbf{Real-time Monitoring}:
\begin{itemize}
\item Scheduler decision latencies
\item Queue depths and wait times
\item CPU utilization and idle statistics
\item Context switch rates and migration counts
\end{itemize}

\textbf{Low-overhead Implementation}:
\begin{itemize}
\item eBPF probes for zero-copy data collection
\item Hardware performance counters
\item Aggregation in kernel space to reduce overhead
\end{itemize}

\subsubsection{Unified Agent Extension Framework}
Standardized interface enabling any AI agent to optimize schedulers:

\textbf{Model Context Protocol (MCP) Server}:
\begin{itemize}
\item RESTful API for workload submission and scheduler retrieval
\item Streaming interface for real-time monitoring data
\item Standardized response formats across different AI models
\end{itemize}

\textbf{Tool Library}:
\begin{itemize}
\item Atomic operations exposed as individual endpoints
\item Composite workflows as higher-level abstractions
\item Custom tool registration for agent-specific needs
\end{itemize}

\textbf{Safety Governance}:
\begin{itemize}
\item Role-based access control for different operations
\item Rate limiting to prevent resource exhaustion
\item Audit logging for all AI actions
\item Resource quotas per agent instance
\end{itemize}

\subsection{RL Algorithm Suite}

\subsubsection{In-Context Learning}
Leverages LLM's ability to learn from examples within the prompt:
\begin{itemize}
\item Performance feedback added to agent context
\item Successful scheduler examples included as references
\item Failed attempts with explanations for learning
\end{itemize}

\subsubsection{Bayesian Optimization}
Efficient parameter tuning with uncertainty quantification:
\begin{itemize}
\item Gaussian process models for parameter-performance relationships
\item Acquisition functions balancing exploration and exploitation
\item Automatic hyperparameter selection
\end{itemize}

\subsubsection{Multi-Armed Bandits}
Scheduler selection under uncertainty:
\begin{itemize}
\item Thompson sampling for probabilistic selection
\item Upper confidence bounds for optimistic exploration
\item Contextual bandits using workload features
\end{itemize}

\subsubsection{Workload Pattern Learning}
Detect and adapt to changing workload phases:
\begin{itemize}
\item Hidden Markov models for phase detection
\item Change point detection algorithms
\item Online clustering of workload behaviors
\end{itemize}

\subsection{Self-Evolution Process}

The system continuously improves through experience:

\textbf{1. Experience Collection}:
\begin{itemize}
\item Every scheduler execution generates performance data
\item Workload characteristics linked to scheduler effectiveness
\item Failed attempts analyzed for root causes
\end{itemize}

\textbf{2. Pattern Extraction}:
\begin{itemize}
\item Successful optimizations become library patterns
\item Common failure modes identified and avoided
\item Cross-workload generalizations discovered
\end{itemize}

\textbf{3. Library Evolution}:
\begin{itemize}
\item New schedulers added when existing ones insufficient
\item Underperforming schedulers deprecated
\item Hybrid schedulers created from successful components
\end{itemize}

\textbf{4. Knowledge Transfer}:
\begin{itemize}
\item Lessons from one workload type applied to similar ones
\item Architecture-specific optimizations shared across schedulers
\item Meta-strategies emerge from accumulated experience
\end{itemize}