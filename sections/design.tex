\section{System Design}
\label{sec:design}

\subsection{Design Principles}

We are building a system interface that can be used by AI agents. Our core insight is that AI agents are fundamentally context engineering systems—they need sufficient information to make decisions but not so much that costs become prohibitive. This mirrors human experts performing optimization: they need the right tools to collect profiling data and implement policies with appropriate frameworks. As system researchers, our goal is not to design better AI agents, but to design better systems and interfaces that can be used by AI agents.

\textbf{Decoupling and Role Separation}: Our first principle separates ``what to optimize'' (AI's domain) from ``how to observe and optimize'' (system's domain). We treat AI agents as performance engineers with clear interface boundaries, where the system provides tools and the AI provides decisions. This evolution-ready design allows future models to leverage the same interfaces without system changes, ensuring our framework remains relevant as AI capabilities advance.

\textbf{Safety-First Interface Design}: We treat AI as a potentially non-cautious actor that can make wrong decisions. Therefore, a core principle is to design interfaces that inherently prevent catastrophic failures. The system must insulate itself from flawed or malicious agent-generated code by default, rather than relying on the agent to be cautious.

\textbf{Context and Feedback Balance}: LLM agents are constrained by context windows, token costs, and the risk of ``losing the signal in the noise.'' Providing too much information is often as detrimental as providing too little. Our core principle is therefore adaptive context provisioning. The framework enables the agent to start with a minimal summary and progressively request more detailed information as needed. This approach creates a crucial trade-off between cost and precision, keeping routine optimizations affordable while enabling deep, data-intensive analysis when required.

\textbf{Composable Tool Architecture}: Following the original Unix philosophy, we believe the framework should not be prescriptive. Our principle is to provide a set of powerful, atomic tools and leave the complex workflow construction and planning to the agent's reasoning capabilities, leveraging its ability to generate novel solutions dynamically.

\subsection{\sys  Architecture}

Our core insight is that safely and efficiently harnessing a powerful LLM agent for OS optimization requires a new architectural pattern. We see the problem as a Reinforcement Learning problem, but we are not building a better AI; we are building a better interface for AI. To that end, we present \sys, a novel control plane designed to intelligently manage the Linux scheduler (the data plane).

In this architecture, the data plane is the Linux kernel itself, where a sched\_ext eBPF scheduler executes at microsecond speeds. The control plane is our framework, operating at a slower timescale to observe, analyze, and safely update the policy running in the data plane. The LLM agent acts as a pluggable, autonomous policy agent within this control plane. The core here is a framework that enables an LLM agent to perform a highly sophisticated form of Reinforcement Learning. If we consider the black-box agent (Claude Code) as the ``core agent'' in an RL paradigm, then the other components of our framework are the essential scaffolding that makes it possible for this agent to operate effectively and safely in the real world.

While the agent operates with full autonomy—choosing which tools to use and when—its workflow is guided by a principled, four-stage control loop: Observation, Planning, Execution, and Learning. This structure provides a scaffold for the agent's reasoning, ensuring its actions are safe, efficient, and effective. The agent is the ``policy agent'' in our framework.

Figure~\ref{fig:architecture} illustrates our system architecture with the production system as the data plane and \sys as the control plane. The agent follows a four-stage workflow: observation (profiling workloads), planning (selecting/adapting schedulers), execution (validating and deploying), and learning (updating the knowledge base).

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\columnwidth}{\centering
\vspace{2cm}
System Architecture Diagram\\
(Data Plane: Linux Kernel with sched\_ext)\\
$\updownarrow$\\
(Control Plane: \sys with Policy Agent)\\
\vspace{2cm}
}}
\caption{\sys Architecture: The data plane (production system) runs sched\_ext schedulers at microsecond speeds, while the control plane (\sys) operates at slower timescales with the LLM policy agent performing observation, planning, execution, and learning cycles.}
\label{fig:architecture}
\end{figure}


\subsubsection{Stage 1: Observation \& Analysis - Building a Workload Profile}

The observation stage enables the policy agent to perform a deep, semantic analysis of a workload and synthesize its findings into a structured ``Workload Profile.'' This profile serves as the foundational understanding and specification for all subsequent optimization steps. The agent pulls the information it needs, from high-level source code to low-level performance counters, ensuring it has the right context without being flooded. \sys provides the agent with access to a secure Analysis Sandbox, a containerized environment where the agent can actively investigate the workload using a wide array of tools, much like a human developer.

Within the sandbox, the agent has multi-modal sensing capabilities including file system access to read source code, Makefiles, dependency manifests, and configuration files to understand the software's structure. Reflecting our composable tool principle, the sandbox provides atomic tools like \texttt{get\_cpu\_stats()} and \texttt{set\_scheduler\_param()}. The system imposes no fixed workflows; the agent dynamically decides tool sequences, enabling it to create new tool combinations for novel scenarios, such as generating custom analysis scripts on demand. It can execute diagnostic shell commands like \texttt{git log}, \texttt{make}, \texttt{perf stat}, and \texttt{strace} to observe the software's build process and runtime behavior. The agent can also call structured metric APIs for real-time performance data. The ultimate output is not raw data but a structured specification file containing a natural language summary of the workload's purpose, key performance characteristics and resource requirements, and explicit optimization goals that guide the next stage. Additionally, the agent can register callback URLs with the \sys Server, allowing our Performance Monitor Daemon to proactively notify the agent of critical events such as performance drops or workload phase changes, triggering new analysis cycles without polling.

\subsubsection{Stage 2: Planning - Policy Synthesis and Selection}

The planning stage uses the Workload Profile generated in Stage 1 to synthesize a concrete optimization plan. The LLM is treated as a pluggable, autonomous agent whose job is to reason about what to do, not the low-level details of how to do it. \sys can work with any capable agent, and the agent's internal Decision Layer uses the rich, natural language goals and requirements from the Workload Profile to guide its strategy.

To act efficiently, the agent typically first queries the Scheduler Knowledge Base, which serves as long-term memory containing a curated collection of production schedulers, a library of algorithm primitives, a suite of RL algorithms for finding parameters, and rich metadata with performance history. Using the profile's keywords to perform accurate semantic searches, the agent autonomously chooses one of three pathways: reconfigure an existing scheduler that is a perfect fit by providing new configuration parameters, revise a close match by retrieving source code and its Implementation Layer generates patches to adapt it, or generate a new scheduler from scratch using algorithm primitives as building blocks when no suitable scheduler exists.

\subsubsection{Stage 3: Execution - Validated Policy Deployment}

The execution stage provides a secure service for the agent to validate and deploy its proposed policy without compromising system stability. This entire stage embodies treating AI as a potentially non-cautious actor, providing non-negotiable validation for all agent-generated code. The Execution Gauntlet is not a passive pipeline but a service the agent explicitly calls.

When the agent has a code artifact, it submits it to the gauntlet, which runs the code through three validation layers. Static pre-flight checks simulate the kernel's BPF verifier to catch safety violations early, analyze instruction counts and code paths to estimate scheduler overhead, and allow the agent to call additional static analysis tools to validate intended behavior. Dynamic sandbox validation compiles and runs the proposed scheduler in a secure sandbox against unit, integration, and stress tests to validate logical correctness, measuring performance to ensure it meets Workload Profile goals before production deployment. For monitored production rollout, if the gauntlet is passed, it returns a deployment token to the agent, who then makes an explicit call to deploy the policy. The Performance Monitor Daemon continuously observes the new scheduler, and if key performance indicators degrade beyond thresholds, the circuit breaker triggers and the system automatically reverts to the last known-good scheduler.

\subsubsection{Stage 4: Learning - Performance Analysis and Knowledge Update}

The learning stage translates the outcome of an action into durable, reusable knowledge, enabling the agent and system to improve over time. This ensures the loop is closed and that the cost of optimization decreases as the system gains experience. Performance metrics gathered after actuation constitute the reward signal, which the agent accesses via the Feedback Channel.

The system processes this signal in two ways. For short-term, in-context learning, the agent calls a \texttt{get\_feedback} tool that returns a concise summary of the outcome, using this feedback to inform its next decision and effectively performing in-context reinforcement learning. For long-term system evolution, structured performance data updates the Scheduler Knowledge Base, refining historical metrics for the scheduler used. Highly successful, novel schedulers generated by the agent can be contributed back to the library, enriching the system's collective intelligence.


\subsection{Example Workflow: Kernel Compilation}

To illustrate how these four stages work together, consider a kernel compilation workload. The agent begins by analyzing the Linux kernel source tree, executing \texttt{make -j} to understand the build process, and running \texttt{perf stat} to profile resource usage. This observation produces a Workload Profile: ``CPU-intensive parallel compilation task with short-lived processes, inter-process dependencies, and a goal to minimize makespan.'' During planning, the agent queries the Knowledge Base with keywords like ``throughput'' and ``compilation,'' retrieving \texttt{scx\_rusty} as a starting point. It generates a patch to make the scheduler dependency-aware. In execution, the agent submits the patched code to the Execution Gauntlet for validation, receiving a deployment token upon success. Finally, after deployment, the agent receives feedback that the revision achieved a 45\% reduction in makespan, contributing the improved scheduler back to the Knowledge Base for future use. This entire workflow demonstrates how \sys enables AI agents to autonomously optimize system performance through iterative refinement.


