\section{\sys{} System Design}

Figure 1 illustrates the \sys{} architecture. The core is an LLM-based agent that synthesizes a scheduler policy, aided by an RL loop for optimization. The process proceeds in two phases:

\begin{itemize}
\item \emph{Phase I: LLM Policy Generation.} The user provides a workload description (e.g. a DAG of tasks, resource profiles, and goals) as input to the LLM prompt. The LLM, guided by a system prompt and possibly example demonstrations, outputs a candidate scheduling policy in our DSL. For instance, the LLM might propose: ``Use HEFT for this DAG, prioritizing longest-path tasks; assign highest CPU jobs to CPU-rich nodes; insert backfilling with priority to I/O-bound jobs.'' In practice, the output is formal DSL code (see below). The DSL we design includes constructs for defining job priorities, resource assignments, backfill rules, and preemption heuristics. This design draws inspiration from prior scheduler DSLs (e.g. Bossa DSL for Linux scheduling and recent RL scheduling frameworks with DSLs). By outputting DSL, the LLM avoids low-level bugs and encodes high-level logic. An excerpt might look like:

\begin{verbatim}
policy {
    tasks: all_ready_tasks;
    priorities: {
        if (task.type == "IO") then priority += 10;
        if (task.critical_path_length > threshold) then priority += 5;
    }
    assign: round_robin_on_idle_CPUs;
    backfill: enable_if(highest_load < X%);
}
\end{verbatim}

\item \emph{Phase II: RL-Based Refinement.} The synthesized policy is instantiated in a simulation environment that models the target system. We employ a fast, event-driven scheduling simulator (e.g., a modified version of \emph{DRAS-CQSim}) seeded with real job traces or synthetic workloads. An RL agent (e.g. Proximal Policy Optimization) is then tasked with \emph{tuning} numerical parameters or picking among policy variants. For example, the LLM might leave some constants (thresholds, weights) unspecified or output multiple strategies; the RL agent explores these via interactions: it applies actions like ``set weight=2 for IO priority'' or ``enable adaptive backfill'', observes simulated makespan and wait times, and obtains a reward (e.g. negative weighted sum of makespan and unfairness). Over successive episodes, the RL agent adjusts the policy to optimize the objective. Because the LLM's output already encodes good heuristics, the RL search is over a much smaller space than beginning from scratch. This idea of \emph{generative initialization plus RL tuning} has proven effective in other domains.
\end{itemize}

\emph{Key Idea \& Observation.} The hybrid approach leverages the LLM's semantic reasoning and the RL's empirical optimization. The LLM agent embeds expert knowledge (e.g. ``IO-bound tasks should not block CPU jobs'') and can rapidly produce a schedule sketch. Then RL fine-tunes continuous parameters and addresses nuances (e.g. the exact preemption interval). This synergy addresses two key challenges of scheduling: the combinatorial policy design (handled by LLM) and the noisy feedback loop (handled by RL).

\emph{Scheduling DSL.} We emphasize the DSL's role. The DSL provides a structured interface between the LLM output and the scheduler implementation. It defines the search space for the agent: only syntactically valid scheduling policies can be generated. Users (or a developer) design the DSL to capture typical scheduling primitives: task queues, priority functions, resource assignment rules, and so on. Because LLMs are good at learning programming-like patterns, this DSL approach greatly simplifies the agent's job. The DSL is compiled or interpreted by a runtime library that interfaces with the simulated or real scheduling environment. For example, a rule like \texttt{prioritize(tasks, by="length", weight=3)} in DSL would translate to C++ or Python code invoked at scheduling time. This was similarly done in an LLM-based mapper generator: Wei \emph{et al.} use a DSL to abstract scheduling code, letting the LLM output high-level mapper strategies.

\emph{Workload Analysis Pre-Run.} A crucial advantage of using an LLM agent is \emph{proactive} workload analysis. Before execution, the LLM can infer workload characteristics from code or descriptions (e.g., ``this is a convolutional neural network training job with short linear algebra tasks and bursty gradient sync'', or ``this pipeline has stage latencies [X,Y,Z]''). It then selects scheduling features (e.g., CPU vs GPU assignment, streaming pipeline priorities). This is akin to ``compiler scheduling'' but done dynamically and adaptively per run. We leverage this to handle heterogeneous task types: for instance, tasks tagged as I/O-intensive get a different policy than compute-bound tasks, automatically inferred by the agent.

\emph{Target System Architecture.} \sys{} is implemented as a \emph{user-level scheduling framework}. It could be integrated into task-based runtimes (e.g. StarPU, Legion, or a container orchestration stack) or as a stand-alone auto-tuning daemon. In our prototype, we simulate a cluster with a custom scheduler backend. In practice, \sys{} would translate DSL policies into calls to the runtime (e.g. hooking into Legion's scheduler or into Kubernetes via a scheduler plugin). This design parallels work like AIOS, which envisions an LLM-aware OS kernel with an ``Agent Scheduler'' module~\cite{arxiv2403}; here, the ``LLM kernel'' would include our policy generator and an RL trainer. Unlike in-kernel schedulers, our user-level approach imposes no kernel modifications and can be adopted incrementally on top of existing systems.

\emph{Generalization and Adaptivity.} By design, \sys{} aims to work \emph{out-of-the-box} on new workloads. The LLM is never retrained per-workload; it relies on its pretraining to handle novel tasks. During testing, we shuffle in completely unseen job mixes and hardware setups. The RL component too can be meta-trained on a variety of job traces, then quickly adapt online. Prior work has shown that RL schedulers can generalize when properly designed, and LLMs further enhance this by transferring knowledge. For example, READYS demonstrated RL generalization to new DAGs, and our LLM agent should improve on that by reasoning at a higher level.

In summary, \sys{}'s architecture synthesizes scheduling policies via LLM-generated DSL code, with RL tuning in the loop. This fulfills the need for automated, workload-aware scheduling that overcomes the limitations of static heuristics and pure RL methods.