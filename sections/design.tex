\section{System Design}

\subsection{Design Principles}

We are building a system interface that can be used by AI agents. As AI becomes more powerful and general, all software we currently design will be used by and maintained by AI in the next few years. Our core insight is that AI agents are fundamentally context engineering systems—they need sufficient information to make decisions but not so much that costs become prohibitive. This mirrors human experts performing optimization: they need the right tools to collect profiling data and implement policies with appropriate frameworks. Choosing the wrong toolset costs significant time when time is money. As system researchers, our goal is not to design better AI agents, but to design better systems and interfaces that can be used by AI agents.

\subsubsection{Decoupling and Role Separation}

Our first principle separates ``what to optimize'' (AI's domain) from ``how to observe and act'' (system's domain). We treat AI agents as performance engineers with clear interface boundaries. The system provides tools and observations, while AI gathers context information and provides decisions and actions. This evolution-ready design allows future models to leverage the same interfaces without system changes, ensuring our framework remains relevant as AI capabilities advance.

\subsubsection{Context and Feedback Balance}

AI agents face a fundamental challenge in balancing information completeness against token costs. Our design addresses this through an adaptive context window that starts with minimal context for simple decisions and progressively adds detail for complex scenarios. The system learns which context matters for which workload types, and adding feedback loops or previous results to the context window helps agents make better decisions. We expose APIs to control information granularity, creating a cost-efficiency trade-off that keeps routine optimizations affordable while enabling detailed analysis when needed.

\subsubsection{Composable Tool Architecture}

Following the original Unix philosophy, we leverage LLMs' code generation abilities through a programmable, not prescriptive approach. Our tool decomposition provides atomic tools for basic operations like \texttt{get\_cpu\_stats()} and \texttt{set\_scheduler\_param()}, which compose into more complex workflows. The system imposes no fixed workflows—AI decides tool sequences dynamically. This enables AI to create new tool combinations for novel scenarios, such as generating custom analysis scripts on demand that profile workloads or compose multiple stat reads into sophisticated analyses.

\subsubsection{Safety-First Interface Design}

We treat AI as a potentially non-cautious actor that can make wrong decisions, designing interfaces that inherently prevent catastrophic failures. Our defensive API design ensures no single call can crash the system, with all operations having built-in bounds and limits that require validation before execution. For example, \texttt{set\_cpu\_limit()} has a hard maximum of 95\% to prevent starvation. We provide staged execution with validation through preview modes, dry-run capabilities for destructive operations, and mandatory validation checks between generation and deployment. The constrained action space whitelists allowed operations rather than blacklisting dangerous ones, with graduated permissions where new AI agents can only tune parameters within 20\% of defaults and earn trust through successful operations. Automatic safety rails enforce resource limits at the system level, with circuit breakers triggering when performance degrades and watchdog timers for all AI-initiated operations. While our goal is a fully automatic system, we maintain human-in-the-loop fallbacks only when necessary, requiring confirmation for critical operations during initial deployments and triggering human review on anomaly detection.

\subsection{\sys: A Control Plane for Autonomous Agent-Driven Scheduling}

Our core insight is that safely and efficiently harnessing a powerful LLM agent for OS optimization requires a new architectural pattern. We see the problem as a Reinforcement Learning problem, but we are not building a better AI; we are building a better interface for AI. To that end, we present \sys, a novel control plane designed to intelligently manage the Linux scheduler (the data plane).

In this architecture, the data plane is the Linux kernel itself, where a sched\_ext eBPF scheduler executes at microsecond speeds. The control plane is our framework, operating at a slower timescale to observe, analyze, and safely update the policy running in the data plane. The LLM agent acts as a pluggable, autonomous policy agent within this control plane. The core here is a framework that enables an LLM agent to perform a highly sophisticated form of Reinforcement Learning. If we consider the black-box agent (Claude Code) as the ``core agent'' in an RL paradigm, then the other components of our framework are the essential scaffolding that makes it possible for this agent to operate effectively and safely in the real world.

While the agent operates with full autonomy—choosing which tools to use and when—its workflow is guided by a principled, four-stage control loop: Observation, Planning, Execution, and Learning. This structure provides a scaffold for the agent's reasoning, ensuring its actions are safe, efficient, and effective. The agent is the ``policy agent'' in our framework.

\subsubsection{Stage 1: Observation \& Analysis - Building a Workload Profile}

The observation stage enables the policy agent to perform a deep, semantic analysis of a workload and synthesize its findings into a structured ``Workload Profile.'' This profile serves as the foundational understanding and specification for all subsequent optimization steps. The agent pulls the information it needs, from high-level source code to low-level performance counters, ensuring it has the right context without being flooded. \sys provides the agent with access to a secure Analysis Sandbox, a containerized environment where the agent can actively investigate the workload using a wide array of tools, much like a human developer.

Within the sandbox, the agent has multi-modal sensing capabilities including file system access to read source code, Makefiles, dependency manifests, and configuration files to understand the software's structure. It can execute diagnostic shell commands like \texttt{git log}, \texttt{make}, \texttt{perf stat}, and \texttt{strace} to observe the software's build process and runtime behavior. The agent can also call structured metric APIs for real-time performance data. The ultimate output is not raw data but a structured specification file containing a natural language summary of the workload's purpose, key performance characteristics and resource requirements, and explicit optimization goals that guide the next stage. Additionally, the agent can register callback URLs with the \sys Server, allowing our Performance Monitor Daemon to proactively notify the agent of critical events such as performance drops or workload phase changes, triggering new analysis cycles without polling.

\subsubsection{Stage 2: Planning - Policy Synthesis and Selection}

The planning stage uses the Workload Profile generated in Stage 1 to synthesize a concrete optimization plan. The LLM is treated as a pluggable, autonomous agent whose job is to reason about what to do, not the low-level details of how to do it. \sys can work with any capable agent, and the agent's internal Decision Layer uses the rich, natural language goals and requirements from the Workload Profile to guide its strategy.

To act efficiently, the agent typically first queries the Scheduler Knowledge Base, which serves as long-term memory containing a curated collection of production schedulers, a library of algorithm primitives, a suite of RL algorithms for finding parameters, and rich metadata with performance history. Using the profile's keywords to perform accurate semantic searches, the agent autonomously chooses one of three pathways: reconfigure an existing scheduler that is a perfect fit by providing new configuration parameters, revise a close match by retrieving source code and its Implementation Layer generates patches to adapt it, or generate a new scheduler from scratch using algorithm primitives as building blocks when no suitable scheduler exists.

\subsubsection{Stage 3: Execution - Validated Policy Deployment}

The execution stage provides a secure service for the agent to validate and deploy its proposed policy without compromising system stability. This entire stage embodies treating AI as a potentially non-cautious actor, providing non-negotiable validation for all agent-generated code. The Execution Gauntlet is not a passive pipeline but a service the agent explicitly calls.

When the agent has a code artifact, it submits it to the gauntlet, which runs the code through three validation layers. Static pre-flight checks simulate the kernel's BPF verifier to catch safety violations early, analyze instruction counts and code paths to estimate scheduler overhead, and allow the agent to call additional static analysis tools to validate intended behavior. Dynamic sandbox validation compiles and runs the proposed scheduler in a secure sandbox against unit, integration, and stress tests to validate logical correctness, measuring performance to ensure it meets Workload Profile goals before production deployment. For monitored production rollout, if the gauntlet is passed, it returns a deployment token to the agent, who then makes an explicit call to deploy the policy. The Performance Monitor Daemon continuously observes the new scheduler, and if key performance indicators degrade beyond thresholds, the circuit breaker triggers and the system automatically reverts to the last known-good scheduler.

\subsubsection{Stage 4: Learning - Performance Analysis and Knowledge Update}

The learning stage translates the outcome of an action into durable, reusable knowledge, enabling the agent and system to improve over time. This ensures the loop is closed and that the cost of optimization decreases as the system gains experience. Performance metrics gathered after actuation constitute the reward signal, which the agent accesses via the Feedback Channel.

The system processes this signal in two ways. For short-term, in-context learning, the agent calls a \texttt{get\_feedback} tool that returns a concise summary of the outcome (e.g., ``Action 'Revise scx\_rusty' resulted in a 45\% reduction in makespan''), using this feedback to inform its next decision and effectively performing in-context reinforcement learning. For long-term system evolution, structured performance data updates the Scheduler Knowledge Base, refining historical metrics for the scheduler used. Highly successful, novel schedulers generated by the agent can be contributed back to the library, enriching the system's collective intelligence.

\subsection{Example Agent Workflow}

An agent's interaction is not a fixed sequence but a dynamic plan. For a kernel compilation task, its plan might proceed as follows. In the observation stage, the agent reads source files, executes \texttt{make -j} to understand build dependencies, runs \texttt{perf stat} to profile the workload, and generates a Workload Profile stating ``This is a CPU-intensive parallel compilation task that benefits from work-stealing and NUMA-aware scheduling. The processes are typically I/O-bound and short-lived. Some processes have dependencies on other processes. The overall goal is to minimize the makespan.'' It then sets up using the \texttt{time} command to collect performance metrics. During planning, it queries the Knowledge Base with keywords like ``throughput'' and ``compilation,'' retrieving \texttt{scx\_rusty}. It decides this is a good starting point but wants to improve it, generating a patch to make it dependency-aware. For execution, it submits the patched code to the Execution Gauntlet. Upon receiving a ``pass'' status and a deployment token, it calls the \texttt{deploy\_policy()} tool. Finally, in the learning stage, after a set time, it calls \texttt{get\_feedback()} and receives a positive reward signal. It then decides to contribute its improved scheduler back to the Knowledge Base for future use.

\subsection{Architecture Overview}

Figure~\ref{fig:architecture} illustrates our system architecture showing the production system as the data plane and \sys as the control plane. An agent's interaction follows a dynamic workflow exemplified by a kernel compilation task: the agent reads source files, executes \texttt{make -j} to understand dependencies, runs \texttt{perf stat} for profiling, and generates a Workload Profile describing it as CPU-intensive parallel compilation benefiting from work-stealing. In planning, it queries the Knowledge Base with relevant keywords, retrieves \texttt{scx\_rusty}, and generates dependency-aware patches. For execution, it submits code to the Execution Gauntlet and deploys upon validation. Finally, it collects performance feedback and contributes improvements back to the Knowledge Base for future use.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\columnwidth}{\centering
\vspace{2cm}
System Architecture Diagram\\
(Data Plane: Linux Kernel with sched\_ext)\\
$\updownarrow$\\
(Control Plane: \sys with Policy Agent)\\
\vspace{2cm}
}}
\caption{\sys Architecture: The data plane (production system) runs sched\_ext schedulers at microsecond speeds, while the control plane (\sys) operates at slower timescales with the LLM policy agent performing observation, planning, execution, and learning cycles.}
\label{fig:architecture}
\end{figure}
