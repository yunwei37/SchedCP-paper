\section{System Design}

\subsection{Design Principles}

We're building a system interface for AI. As AI agents become more powerful, all software we design will be used and maintained by AI in coming years.

Our core insight: AI agents are context engineering systems—they need enough information for decisions without prohibitive costs. This mirrors human expert optimization: using right tools for profiling and implementing policies with appropriate frameworks. Wrong toolsets waste time and money. As system researchers, we design better systems and interfaces for AI agents, not better AI agents.

\subsubsection{Decoupling and Role Separation}
Our first principle separates ``what to optimize'' (AI's domain) from ``how to observe and act'' (system's domain). We treat AI agents as performance engineers with clear interface boundaries. AI agents understand workload patterns, identify optimizations, and generate strategies. The system provides observations, executes actions safely, and enforces constraints. This evolution-ready design lets future AI models use the same interfaces without system redesign. As AI improves, the system benefits without architectural changes, maintaining stability while intelligence evolves.

\subsubsection{Context and Feedback Balance}
AI agents trade off information completeness against token costs. Our adaptive context window starts minimal for simple decisions and adds detail for complex scenarios. The system learns which context matters for which workloads, building relevant information patterns. Integrating previous results enables experiential learning. Cost-aware APIs control information granularity, adjusting detail by decision importance. Routine optimizations stay cost-effective while complex scenarios get detailed analysis.

\subsubsection{Composable Tool Architecture}
Following Unix philosophy, we leverage LLM code generation through programmable, composable tools. Atomic tools provide basic operations like \texttt{get\_cpu\_stats()} and \texttt{set\_scheduler\_param()}. These combine into compositional tools for complex workflows. AI dynamically creates new tool combinations and generates custom analysis scripts on demand, adapting to unexpected patterns without manual intervention.

Atomic tools compose into sophisticated profiling functions tracking workload phases and patterns. AI might generate analysis detecting compilation patterns and prioritizing by dependency graphs—optimizations hard to anticipate. This composable architecture grows capabilities with each workload, building an expanding optimization toolkit.

\subsubsection{Safety-First Interface Design}
We treat AI as potentially non-cautious actors needing defensive interfaces. Multiple protection layers: defensive API design ensures no call crashes the system, with built-in bounds and limits. \texttt{set\_cpu\_limit()} caps at 95\% preventing CPU starvation even with unreasonable AI requests.

Staged execution provides safety: preview modes show effects before execution, dry-runs test destructive operations. Mandatory validation between generation and deployment catches issues before production impact. Constrained action space whitelists safe operations. New agents start limited (20\% parameter variations), earning autonomy through success.

Automatic safety rails enforce resource limits regardless of AI requests. Circuit breakers activate on performance degradation, watchdog timers bound operations, audit trails enable debugging. Even with poor AI decisions, systems remain stable and recoverable.

\subsection{System Architecture}

\subsubsection{Multi-Layer RL LLM Agent}
Our agent has three specialized layers. The Decision Layer analyzes workloads and system state to select strategies: configure existing schedulers, modify them, or create new ones. It reasons about trade-offs considering complexity, performance gains, and time constraints.

The Implementation Layer translates decisions into code. It generates eBPF C for kernel components and Rust for userspace, following sched\_ext conventions and safety requirements. This handles kernel programming complexities, letting Decision Layer focus on strategy.

The Learning Layer applies RL for continuous improvement. It processes scheduler feedback, updates strategy selection from outcomes, and maintains context memory. Each deployment enriches understanding of what works for different workloads.

\subsubsection{AI-Managed Scheduler Library}
The scheduler library is institutional memory and reusable component repository. Each entry records scheduler characteristics and performance history. Natural language descriptions summarize purpose and use cases for AI understanding. Configuration parameters have ranges and defaults for safe fine-tuning.

Entries include complete eBPF kernel and userspace source code for immediate deployment or modification. Performance history provides empirical evidence across workload types. Test results with verification status and benchmarks ensure only validated schedulers deploy.

Example: compilation-aware scheduler prioritizes by dependency depth, achieving 1.8x kernel build speedup across 50 samples. Rich metadata enables intelligent selection and configuration by workload requirements.

\subsubsection{Static Analysis \& Testing Framework}
Our framework validates safety-critically before deployment. BPF verification pipeline pre-validates against kernel rules, checking for infinite loops and invalid memory access. It estimates verification complexity avoiding kernel loading timeouts, giving AI early code viability feedback.

Performance estimation uses static analysis predicting scheduler overhead. It analyzes instruction counts, branch complexity, cache patterns identifying bottlenecks. Early feedback helps AI refine implementations before production impact.

Automated testing validates through multiple layers: unit tests verify functions, integration tests validate with simulated workloads, stress tests explore edge cases. Multi-tiered testing catches condition-specific issues ensuring robust implementations.

\subsubsection{Scheduler Optimization Libraries}
Optimization libraries provide pre-built patterns and primitives for rapid development. Production schedulers include: \texttt{scx\_rusty} (work-stealing with load balancing), \texttt{scx\_layered} (hierarchical with cgroup awareness), \texttt{scx\_central} (centralized queue for NUMA). These serve as deployment options and AI learning examples.

Algorithm primitives offer building blocks: FIFO, LIFO, priority queues. Includes CFS-style virtual runtime for fairness, work-stealing/migration for load balancing, CPU affinity for NUMA. Primitives combine and customize for novel algorithms tailored to workloads.

Smart retrieval enables efficient usage: semantic search with natural language, performance-based ranking from historical data. System extracts successful patterns from AI schedulers, continuously enriching the library with discovered optimizations.

\subsubsection{Profiling \& Monitoring Toolkit}
Our toolkit provides workload analysis and observability for scheduler optimization. Workload profiling captures CPU patterns, detects phase transitions between compute and I/O behaviors. Analyzes memory patterns and working sets for cache requirements. Monitors I/O behavior and syscalls for communication patterns. Task creation rates identify batch vs interactive workloads.

Real-time monitoring tracks scheduler metrics: decision latencies, queue depths, wait times. Monitors CPU utilization and idle stats for inefficiencies. Tracks context switches and migrations for overhead. Real-time data enables anomaly detection and immediate feedback.

Low-overhead implementation prevents monitoring impact. eBPF probes enable zero-copy kernel collection, hardware counters provide cycle-accurate measurements, in-kernel aggregation reduces data volume. Enables continuous production monitoring without performance impact.

\subsubsection{Unified Agent Extension Framework}
Our framework standardizes interfaces for any AI agent optimizing schedulers, from open-source to proprietary. MCP server implements RESTful API for workload submission and scheduler retrieval, with streaming for real-time monitoring. Standardized formats ensure cross-model compatibility, allowing AI provider switches without system changes.

Tool library exposes atomic operations as endpoints and composite workflows as abstractions. Simple agents use high-level tools while sophisticated ones compose custom workflows from atomic operations. Framework supports custom tool registration for agent-specific extensions without core modifications.

Safety governance ensures secure multi-tenant operation: role-based access restricts by agent capabilities and trust. Rate limiting prevents runaway resource exhaustion. Audit logging tracks AI actions for debugging/compliance. Resource quotas ensure fair sharing and prevent monopolization.

\subsection{Reinforcement Learning Integration}

\subsubsection{In-Context Learning}
We leverage LLMs' ability to learn from context window examples. Performance feedback from deployments automatically enters agent context, creating dynamic learning. Successful schedulers become reference examples showing what works per workload. Failed attempts include explanations of problems. AI learns from successes and failures without retraining, adapting strategies from accumulated session experience.

\subsubsection{Bayesian Optimization}
Bayesian optimization efficiently tunes parameters with uncertainty quantification. Gaussian processes learn parameter-performance relationships, modeling how scheduler parameters affect workloads. Acquisition functions balance exploring uncertain regions with exploiting known good configs. Automatic hyperparameter selection adapts optimization, tuning exploration-exploitation trade-offs based on performance variability. This minimizes configuration attempts for optimal parameters.

\subsubsection{Multi-Armed Bandits}
Multi-armed bandits address scheduler selection under uncertainty. Thompson sampling selects probabilistically from performance distributions, balancing untested scheduler exploration with proven performer exploitation. Upper confidence bounds optimistically select schedulers that might perform well given uncertainty. Contextual bandits incorporate workload features, learning which scheduler traits correlate with workload success. This creates adaptive selection improving with experience while maintaining performance.

\subsubsection{Workload Pattern Learning}
We detect and adapt to changing workload phases in complex, time-varying applications. Hidden Markov models identify phases and predict transitions for proactive scheduler adjustments. Change point detection identifies characteristic shifts, triggering strategy re-evaluation. Online clustering groups similar behaviors, building dynamic workload pattern taxonomy for recognizing recurring scenarios. This ensures optimal performance as workloads evolve during execution.

\subsection{Continuous Learning and Evolution}

The system continuously improves through structured self-evolution transforming individual experiences into collective intelligence. Experience collection links workload characteristics to scheduler effectiveness, understanding what works when and why. Failed attempts get root cause analysis, learning from mistakes.

Pattern extraction transforms experience into reusable knowledge. Successful optimizations become library patterns for new scenarios. System identifies failure modes and builds defensive strategies. Cross-workload generalizations emerge from broadly applicable principles.

Library evolution keeps schedulers relevant. New schedulers added when existing ones insufficient. Underperformers deprecated by empirical evidence. System creates hybrids combining successful components, exploring new designs through synthesis.

Knowledge transfer amplifies learning value. Lessons from one workload apply to similar scenarios. Architecture-specific optimizations share across library. Meta-strategies emerge from accumulated experience guiding future optimization.

\subsection{Architecture Overview}

Our architecture separates production from AI agent systems through defined interfaces. Production runs workloads under Linux kernel with sched\_ext, performance monitor daemon collects metrics. MCP server interfaces production and AI, exposing workload analysis and scheduler deployment tools.

AI agent system has: multi-layer RL LLM agent making decisions, scheduler library storing components, safety validation ensuring correctness. Modular design allows independent evolution maintaining stability. Production communicates via standardized APIs for seamless deployment, monitoring, and feedback without disrupting workloads.

Agent workflow: (1) analyze workload characteristics, (2) search library for suitable schedulers, (3) decide to configure/modify/create schedulers, (4) validate safety and correctness, (5) deploy to production, (6) collect feedback for improvement. This ensures safe and effective optimization.

Figure~\ref{fig:architecture} shows a placeholder for our system architecture diagram, illustrating how the production system interfaces with the AI agent system through the MCP server.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\columnwidth}{\centering
\vspace{2cm}
System Architecture Diagram\\
(Production System $\leftrightarrow$ MCP Server $\leftrightarrow$ AI Agent System)\\
\vspace{2cm}
}}
\caption{System Architecture: Production system (top) interfaces with AI agent system (bottom) through MCP server. The AI agent selects, modifies, or generates schedulers based on workload analysis, with all code passing through safety validation before deployment.}
\label{fig:architecture}
\end{figure}