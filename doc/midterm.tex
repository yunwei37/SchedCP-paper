\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{cite}
\usepackage{geometry}
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=2.5cm,
    bottom=2.5cm
}

\title{SchedCP: Autonomous OS Optimization with LLM Agents}
\author{Yusheng Zheng}
% \date{\today}

\begin{document}

\maketitle

\section{Introduction and Background}

This work presents our approach to autonomous OS optimization using LLM agents, focusing on schedulers as a critical component. This section revisits the problem and projects we are working on, and the later part will focus on the benchmark design and implementation we are building.

Operating systems face semantic and knowledge gaps limiting performance optimization. Schedulers cannot distinguish between latency-sensitive and throughput-oriented workloads, while developers lack workload insight and users lack kernel expertise. Traditional RL-based methods require per-workload training and human-specified SLOs. Naïve LLM approaches employ fixed pipelines, operate unsafely, and prove inefficient with costs reaching \$6 and 33 minutes per run. SchedCP separates AI reasoning from system execution through two stages: goal-inference analyzes workload intent, and policy-synthesis generates safe eBPF schedulers.

The MCP server incorporates a Workload Analysis Engine, Policy Repository for eBPF templates, and Execution Verifier for safety checks. The agent implements a four-phase cycle: observation, planning, execution, and learning. This positions the LLM in the control plane, managing the OS without runtime overhead.

SchedCP achieved 1.79× faster execution, 2.11× lower P99 latency, 1.60× higher throughput, and 13× cost reduction versus naïve implementations, validated on kernel builds and schbench. Further development includes standardized benchmarks and extending beyond CPU scheduling to I/O, memory, and power subsystems.

\section{Benchmark Framework Design}

The benchmark framework evaluates LLM agents' ability to optimize OS behavior for diverse workloads under explicit SLOs and budgets, including time, tokens, CPU usage, and energy consumption. This framework builds a reinforcement learning-like environment for agents, allowing them to tune OS configurations and use code generation to alter OS behavior.

\textbf{Research Questions.} Our benchmark framework addresses four key research questions:

\begin{enumerate}
    \item \textbf{Goal Inference from Telemetry:} Can agents autonomously infer optimization goals from system telemetry without being explicitly told the Service Level Objectives (SLOs)? This includes analyzing performance metrics, system logs, and traces to identify bottlenecks and determine appropriate optimization targets based on workload characteristics.

    \item \textbf{SLO Maintenance and Stability:} Can agents maintain SLOs under drift with controller-grade stability while avoiding thrashing behavior? This involves adapting to changing workload patterns, maintaining consistent performance under varying loads, and preventing oscillations between different optimization strategies that could degrade system performance.

    \item \textbf{Efficiency and Scaling Laws:} What improvement do we achieve per token or second, and what are the scaling laws governing this relationship? This examines the cost-effectiveness of AI-driven optimization, including token consumption rates, time to optimization convergence, and the relationship between model size, computational resources, and optimization quality.

    \item \textbf{Optimization Strategies and System Interfaces:} How do agents achieve optimizations, and do we need to design better system interfaces to facilitate their work? This investigates the specific techniques agents employ, whether current OS interfaces are sufficient for AI-driven optimization, and what new abstractions or APIs might enable more effective autonomous system management.
\end{enumerate}

\textbf{Task Design: Two-Phase Challenge.} The benchmark implements a two-phase challenge structure. During goal inference, agents must analyze traces, metrics, and logs to infer bottlenecks and optimization targets. The policy and tool synthesis phase requires agents to select and configure existing tools or synthesize new code, particularly eBPF schedulers, to meet specified SLOs. This design allows testing with different models and agents, including Claude code and Codex implementations.

\section{Benchmark Implementation}

Our implementation employs open-source software using Helm and Docker with pre-defined SLOs derived from documentation.

\textbf{Workload Suite.} The comprehensive workload suite includes 20-30 different applications categorized by their characteristics. CPU-bound workloads include kernel builds, LLVM compilation, compression tools like xz and gzip, and ffmpeg encoding. Latency-critical workloads encompass schbench, hackbench, and context-switch benchmarks. Server workloads combine nginx with wrk and Redis with memtier for realistic service scenarios. Data processing tasks include sort and join operations alongside SQLite queries. Stress testing covers memory and CPU test suites. GPU workloads incorporate vllm, llama.cpp, and PyTorch applications. Each workload includes clear SLOs and repeatable harness configurations.

\textbf{Baselines and Infrastructure.} We compare against three baselines: Linux defaults using CFS/EEVDF with tuned profiles, published RL/ML schedulers from recent literature, and naïve LLM agents without control plane access but with bash capabilities.

The infrastructure comprises several components. The runner operates in containers or VMs with pinned kernel versions for consistency. An agent sandbox provides the MCP server and verifier for safe experimentation. The evaluator performs multi-run statistics and SLO verification. Reproducibility is ensured through fixed random seeds and consistent hardware profiles.

Currently, we have built a framework incorporating approximately 10 workloads, with ongoing work to expand this suite. The complete implementation is available at \url{https://github.com/eunomia-bpf/schedcp}.

\section*{References}

Zheng et al., ``Towards Agentic OS: An LLM Agent Framework for Linux Schedulers,'' MLforSystem 2025 workshop, NIPS. Available at: \url{https://arxiv.org/html/2509.01245v4}

\end{document}