# **Sched‑Agent: AI Agent–Guided Dynamic Scheduler Synthesis and Configuration**

## **Abstract**

Modern high-performance computing (HPC) workloads and cloud services exhibit highly dynamic and heterogeneous characteristics that defeat one-size-fits-all static schedulers. Manually designing and tuning scheduling policies for each workload is laborious and brittle. We propose **Sched-Agent**, an AI-driven framework that **automatically generates and configures dynamic schedulers** tailored to each workload. Our system leverages a large language model (LLM) as an intelligent agent to **analyze workload descriptions and system goals**, then emits scheduling policies in a domain-specific language (DSL). These policies are further refined via reinforcement learning (RL) fine-tuning. Sched-Agent requires no manual expert input or workload-specific training: the LLM’s reasoning capability identifies workload patterns and generates executable scheduling strategies. Using diverse benchmarks (HPC kernels, dataflow DAGs, and agent-based workloads), we show that Sched-Agent consistently outperforms traditional schedulers and RL-only methods, while generalizing across workloads without retraining. Our contributions include (1) a novel agent-based scheduler synthesis framework combining LLM reasoning and RL tuning, (2) a scheduling DSL bridging LLM output and system execution, and (3) a comprehensive evaluation demonstrating improved makespan, utilization, and fairness across varied workloads.

## **1. Introduction**

Efficient scheduling is critical for performance in multi-tenant HPC clusters and cloud datacenters, where jobs compete for CPUs, GPUs, memory, and I/O resources. Traditional schedulers (e.g. SLURM, PBS, Torque) assume jobs have **fixed resource demands and durations**. For example, Slurm’s batch scheduler allocates resources based on user-specified estimates, leading to long queues and low utilization when workloads vary unexpectedly[arxiv.org](https://arxiv.org/html/2401.09706v1#:~:text=utilization,pushing%20for%20new%20scheduling%20techniques). In contrast, modern cloud schedulers (e.g. Kubernetes, Borg) allow elastic scaling, but HPC environments lack such dynamic adaptability, often reporting dozens of minutes of average wait time for jobs and low overall resource utilization[arxiv.org](https://arxiv.org/html/2401.09706v1#:~:text=utilization,pushing%20for%20new%20scheduling%20techniques). Meanwhile, emerging AI-powered workloads (e.g. large-scale training, inference chains) and heterogeneous architectures demand **multi-objective scheduling** (latency vs throughput vs energy) and fine-grained resource awareness. Manually designing scheduling heuristics or tuning parameters for each scenario is prohibitively expensive and error-prone.

Recent research has explored machine learning for scheduling. Reinforcement learning (RL) agents have been shown to **learn scheduling policies** directly from job traces. For instance, *Decima* uses RL on graph neural networks to schedule batch jobs in data clusters, improving throughput and makespan over heuristic baselines. *DRAS* employs deep RL for HPC batch scheduling with job reservation and backfilling, achieving up to 45% reduction in job completion time versus static policies. Other works combine graph neural nets and RL for DAG and resource co-scheduling, yielding significant utilization gains. However, RL approaches require extensive training for each environment and may struggle with new workload patterns or multiple scheduling objectives.

Meanwhile, **LLM-based agents** have demonstrated remarkable reasoning abilities in complex tasks. Even under zero-shot prompting, LLMs can produce reasonable initial solutions for scheduling problems. For example, an LLM (e.g. GPT-4) can cluster and assign conference papers to sessions, creating draft schedules that are often only a few adjustments away from human quality. In HPC scheduling, recent work shows that an LLM can reason through multi-objective scheduling scenarios (minimize makespan, wait time, etc.) using a ReAct (reason+act) framework, balancing goals without domain-specific training. These results suggest that LLMs can generalize scheduling knowledge from text and math contexts to new workloads.

Our insight is that an LLM agent can **automatically analyze a workload and generate a custom scheduler** in a high-level form, which is then enacted or fine-tuned by RL. By treating scheduling as an AI planning problem, Sched-Agent can leverage chain-of-thought reasoning and vast pretraining to capture domain heuristics, while RL handles environment-specific optimization. Concretely, the Sched-Agent workflow is: (1) **Workload Analysis**: the LLM reads a description of the tasks (program code snippets, performance traces, system constraints) and identifies key properties (e.g. task DAG structure, compute vs I/O intensity, real-time demands). (2) **Policy Synthesis**: using a **scheduling DSL**, the agent generates a candidate scheduling policy or parameter configuration. (3) **RL Fine-tuning**: an RL loop simulates the proposed scheduler on the workload; performance feedback is used to refine the policy (e.g. adjusting priorities or parameters). This closed loop continues until convergence. Crucially, the LLM’s plan serves as a strong initialization that guides RL, requiring far fewer iterations than blind search.

Sched-Agent’s generality and automatic nature address several challenges:

- **Diverse Workloads**: By analyzing workload semantics, the LLM tailors scheduling to each scenario, from CPU-bound compute jobs to I/O-heavy pipelines, without manual reprogramming. We evaluate on a broad suite of benchmarks (HPC linear algebra DAGs, multi-stage dataflows, LLM inference jobs) to test generalization.
- **Complex Objectives**: The system can incorporate multi-objective goals (throughput, latency, fairness) into the agent prompt, letting the LLM reason about trade-offs.
- **Explainability**: The LLM can produce human-readable justifications for scheduling decisions (via chain-of-thought), aiding debugging and trust in mission-critical settings.
- **DSL Integration**: We define a small **domain-specific language** for scheduling policies (inspired by prior scheduler DSLs). The LLM outputs code or configuration in this DSL, which is then compiled into the system. The DSL abstracts low-level details while remaining expressive enough to capture policies.

In summary, we contribute: **(1)** The design of *Sched-Agent*, a new framework combining LLM reasoning with reinforcement learning to generate dynamic schedulers. **(2)** A scheduling DSL and Agent-System interface that enables the LLM to emit executable policies while simplifying RL tuning. **(3)** A comprehensive evaluation on varied workloads showing Sched-Agent improves scheduling performance over static heuristics and pure-RL baselines, and generalizes across tasks.

## **2. Motivation and Challenges**

High-performance systems face **stochastic and evolving workloads**. Jobs arrive with uncertain run times, memory footprints, and interdependencies. Static schedulers (FCFS, fixed-priority, hand-tuned backfilling) cannot adapt in real time. For example, Horizon2022 reports that many HPC centers have less than 50% utilization due to fixed allocations and batch wait time. In contrast, cloud clusters use autoscaling and elastic scheduling to handle variability[arxiv.org](https://arxiv.org/html/2401.09706v1#:~:text=utilization,pushing%20for%20new%20scheduling%20techniques). There is thus a pressing need for **automated scheduler adaptation**.

While RL has shown promise, it alone faces challenges in our setting. RL agents need many simulated jobs to train and often overfit to specific workload distributions. The state and action spaces in scheduling are huge (all pending jobs × possible allocations), leading to sample inefficiency. Moreover, RL typically outputs opaque neural policies with no rationale. On the other hand, LLMs bring **prior knowledge** from vast corpora, including domain heuristics and system documentation. They can “understand” a workload description and propose strategies in natural language or code. Recent work on RL scheduler DSLs shows the power of structured policy representations. Our key observation is that LLMs can kick-start scheduling design, reducing RL training burden, while RL can fine-tune details (like numeric parameters).

However, combining LLMs and RL for scheduling raises new challenges:

- **Workload Description**: What input to give the LLM? We provide a concise summary of workload features (e.g. job DAG topology, average CPU vs IO usage, deadlines) in natural language or JSON. The LLM must parse this and map it to scheduling actions.
- **Action Representation**: The LLM must output a scheduler policy in a well-defined form. We introduce a DSL (Section 3) for policies (e.g. list-scheduling with priority functions, neural policy rules, or solver calls). Designing this DSL is nontrivial: it must balance expressiveness (to capture complex strategies) with simplicity (so LLMs and RL can handle it).
- **RL Reward and Stability**: We must define reward functions that capture scheduling goals (makespan, slowdown, fairness) and incorporate them into RL. Multi-objective rewards are supported via weighted sums or multi-head networks. We also need variance reduction techniques to train stably on bursty workloads, as prior work notes high variance in HPC scheduling simulations.
- **Integration with Systems**: Sched-Agent is designed as a **user-level runtime module** rather than OS kernel code. Many task-based runtimes (Legion, StarPU, OpenMP runtime) allow custom schedulers[arxiv.org](https://arxiv.org/html/2404.03226v1#:~:text=characteristics,grained%20tasks%20and). We implement Sched-Agent as a plugin or user-space library. This decoupling eases deployment but means the agent controls only user-space scheduling; it must coexist with system-level CPU allocation. We assume the cluster resource allocation (nodes to jobs) is fixed by an outer scheduler (e.g. Slurm), and Sched-Agent optimizes job ordering, task placement, and prioritization within that job set.
- **Generalization**: The system should perform well on new workloads unseen in training. We therefore design the agent to rely on LLM generalization (zero-shot reasoning) and on meta-trained RL (e.g., training RL on a distribution of workloads). We will validate generalization on a held-out set of benchmarks.

## **3. Sched-Agent System Design**

Figure 1 illustrates the Sched-Agent architecture. The core is an **LLM-based agent** that synthesizes a scheduler policy, aided by an **RL loop** for optimization. The process proceeds in two phases:

- **Phase I: LLM Policy Generation.** The user provides a workload description (e.g. a DAG of tasks, resource profiles, and goals) as input to the LLM prompt. The LLM, guided by a system prompt and possibly example demonstrations, outputs a candidate scheduling policy in our DSL. For instance, the LLM might propose: “Use **HEFT** for this DAG, prioritizing longest-path tasks; assign highest CPU jobs to CPU-rich nodes; insert backfilling with priority to I/O-bound jobs.” In practice, the output is formal DSL code (see below). The DSL we design includes constructs for defining job priorities, resource assignments, backfill rules, and preemption heuristics. This design draws inspiration from prior scheduler DSLs (e.g. Bossa DSL for Linux scheduling and recent RL scheduling frameworks with DSLs). By outputting DSL, the LLM avoids low-level bugs and encodes high-level logic. An excerpt might look like:
    
    ```
    policy {
        tasks: all_ready_tasks;
        priorities: {
            if (task.type == "IO") then priority += 10;
            if (task.critical_path_length > threshold) then priority += 5;
        }
        assign: round_robin_on_idle_CPUs;
        backfill: enable_if(highest_load < X%);
    }
    
    ```
    
- **Phase II: RL-Based Refinement.** The synthesized policy is instantiated in a simulation environment that models the target system. We employ a fast, event-driven scheduling simulator (e.g., a modified version of *DRAS-CQSim*) seeded with real job traces or synthetic workloads. An RL agent (e.g. Proximal Policy Optimization) is then tasked with *tuning* numerical parameters or picking among policy variants. For example, the LLM might leave some constants (thresholds, weights) unspecified or output multiple strategies; the RL agent explores these via interactions: it applies actions like “set weight=2 for IO priority” or “enable adaptive backfill”, observes simulated makespan and wait times, and obtains a reward (e.g. negative weighted sum of makespan and unfairness). Over successive episodes, the RL agent adjusts the policy to optimize the objective. Because the LLM’s output already encodes good heuristics, the RL search is over a much smaller space than beginning from scratch. This idea of *generative initialization plus RL tuning* has proven effective in other domains.

**Key Idea & Observation.** The hybrid approach leverages the **LLM’s semantic reasoning** and the **RL’s empirical optimization**. The LLM agent embeds expert knowledge (e.g. “IO-bound tasks should not block CPU jobs”) and can rapidly produce a schedule sketch. Then RL fine-tunes continuous parameters and addresses nuances (e.g. the exact preemption interval). This synergy addresses two key challenges of scheduling: the **combinatorial policy design** (handled by LLM) and the **noisy feedback loop** (handled by RL).

**Scheduling DSL.** We emphasize the DSL’s role. The DSL provides a structured interface between the LLM output and the scheduler implementation. It defines the search space for the agent: only syntactically valid scheduling policies can be generated. Users (or a developer) design the DSL to capture typical scheduling primitives: task queues, priority functions, resource assignment rules, and so on. Because LLMs are good at learning programming-like patterns, this DSL approach greatly simplifies the agent’s job. The DSL is compiled or interpreted by a runtime library that interfaces with the simulated or real scheduling environment. For example, a rule like **`prioritize(tasks, by="length", weight=3)`** in DSL would translate to C++ or Python code invoked at scheduling time. This was similarly done in an LLM-based mapper generator: Wei *et al.* use a DSL to abstract scheduling code, letting the LLM output high-level mapper strategies.

**Workload Analysis Pre-Run.** A crucial advantage of using an LLM agent is *proactive* workload analysis. Before execution, the LLM can infer workload characteristics from code or descriptions (e.g., “this is a convolutional neural network training job with short linear algebra tasks and bursty gradient sync”, or “this pipeline has stage latencies [X,Y,Z]”). It then selects scheduling features (e.g., CPU vs GPU assignment, streaming pipeline priorities). This is akin to “compiler scheduling” but done dynamically and adaptively per run. We leverage this to handle heterogeneous task types: for instance, tasks tagged as I/O-intensive get a different policy than compute-bound tasks, automatically inferred by the agent.

**Target System Architecture.** Sched-Agent is implemented as a *user-level scheduling framework*. It could be integrated into task-based runtimes (e.g. StarPU, Legion, or a container orchestration stack) or as a stand-alone auto-tuning daemon. In our prototype, we simulate a cluster with a custom scheduler backend. In practice, Sched-Agent would translate DSL policies into calls to the runtime (e.g. hooking into Legion’s scheduler or into Kubernetes via a scheduler plugin). This design parallels work like AIOS, which envisions an LLM-aware OS kernel with an “Agent Scheduler” module[arxiv.org](https://arxiv.org/html/2403.16971v1#:~:text=Agent%20Scheduler%3A%20Prioritizes%20and%20schedules,requests%20to%20optimize%20LLM%20utilization); here, the “LLM kernel” would include our policy generator and an RL trainer. Unlike in-kernel schedulers, our user-level approach imposes no kernel modifications and can be adopted incrementally on top of existing systems.

**Generalization and Adaptivity.** By design, Sched-Agent aims to work *out-of-the-box* on new workloads. The LLM is never retrained per-workload; it relies on its pretraining to handle novel tasks. During testing, we shuffle in completely unseen job mixes and hardware setups. The RL component too can be meta-trained on a variety of job traces, then quickly adapt online. Prior work has shown that RL schedulers can generalize when properly designed, and LLMs further enhance this by transferring knowledge. For example, READYS demonstrated RL generalization to new DAGs, and our LLM agent should improve on that by reasoning at a higher level.

In summary, Sched-Agent’s architecture synthesizes scheduling policies via LLM-generated DSL code, with RL tuning in the loop. This fulfills the need for **automated, workload-aware scheduling** that overcomes the limitations of static heuristics and pure RL methods.

## **4. Implementation**

We built a prototype of Sched-Agent using off-the-shelf LLMs (e.g. GPT-4/Gemini) and an RL toolkit (OpenAI Gym and Stable Baselines3). The **DSL** is implemented as a small Python-embedded language: users (or the agent) write policy rules in a YAML-like syntax, which are loaded into the scheduler. Our DSL supports: (1) *priority rules* (conditional statements adjusting task priority), (2) *resource assignments* (e.g. pin GPU-only tasks to GPUs), (3) *backfill strategies* (enable/disable, with thresholds), and (4) *preemption and promotion rules* for latency-sensitive tasks. The DSL is strongly typed to avoid invalid policies.

The **agent-system interface** follows the pattern of Wei *et al.*. The LLM agent prompt includes a description of the DSL grammar and several annotated examples of policies for common cases. When invoked, the agent outputs a policy in plain text which our parser turns into an internal policy object. We experimented with *ReAct-style prompting* to have the LLM produce reasoning steps along with final policy lines, which improved policy quality and interpretability.

For **RL training**, we extended an event-driven scheduler simulator (modeled after production systems like SLURM) to accept DSL policies as initial policies and let the agent adjust parameters. We defined the **state** as a summary vector of (pending queue lengths, current resource utilization, job wait times) and the **actions** as adjusting numeric DSL parameters or toggling rules. The **reward** is a weighted combination of negative makespan, average wait time, and a fairness penalty. To stabilize training on bursty trace data, we adapted the trajectory filtering technique from RLScheduler and the uncertainty-aware approach from *Cilantro*, allowing the agent to explore safely.

We consider a variety of **workload inputs**: job traces from real HPC centers (PIK-IPLEX, SDSC [15]), synthetic DAGs (Cholesky, QR) used in READYS, streaming dataflow benchmarks, and LLM inference chains. The system is flexible enough to ingest different descriptors: e.g., a DAG file for task graphs, or a summarized cluster log for batch scheduling.

Finally, Sched-Agent can operate in **online mode**: after initial synthesis/tuning, it monitors actual job execution and can re-invoke the agent if patterns change (new job types appear). This enables continuous adaptation. Our prototype thus spans LLM prompting, DSL parsing, simulation, and RL, demonstrating the feasibility of the concept.

## **5. Evaluation Methodology**

We evaluate Sched-Agent on the following research questions:

1. **Performance Improvement:** *Can Sched-Agent reduce makespan and wait time compared to baselines?* We compare against classic heuristics (FCFS with EASY Backfilling, CFS-like round-robin) and state-of-the-art RL schedulers (Decima-style, DRAS).
2. **Generalization:** *Does Sched-Agent work on new, unseen workloads?* We test on a mixture of benchmark suites not used during any tuning: different DAG structures, job mixes, hardware configurations. We measure performance degradation if any and success rate of the LLM step in generating valid policies across domains.
3. **Ablations – LLM vs RL:** *What is the value of the LLM agent vs pure RL?* We run variants: (a) **LLM-only**: use the generated policy without RL refinement, (b) **RL-only**: a conventional RL agent without the LLM-provided prior, (c) **LLM+RL** (full Sched-Agent). This quantifies how much LLM bootstraps improvement.
4. **Adaptivity:** *Can Sched-Agent adapt to changing workloads?* We simulate workload shifts (e.g. sudden introduction of I/O-heavy jobs) and observe how quickly Sched-Agent reconfigures itself, comparing to static baselines and retrained RL.
5. **Overhead:** *What is the runtime overhead of using an LLM and RL?* We measure the time for policy generation and RL iterations, and show this overhead is modest compared to job runtimes (suitable for offline or low-frequency tuning).

**Workloads and Metrics.** We use diverse benchmarks: HPC DAGs (e.g. Cholesky, LU from [49]), dataflow graphs, and real job traces from supercomputers. Metrics include *average job completion time*, *makespan*, *cluster utilization*, and *fairness*. For multi-tenant settings, we also track *Jain’s fairness index*.

**Baselines.** In addition to standard schedulers, we compare to specialized systems: *Llumnix* for LLM-serving latency scheduling, *Agent.xpu* for on-device LLM kernels (to gauge how domain-specific schedulers behave), and *AutoSched* (the closest auto-tuning framework[tianweiz07.github.io](https://tianweiz07.github.io/Papers/24-ics-1.pdf#:~:text=To%20address%20this%20dilemma%2C%20we,We%20showcase%20how%20AutoSched%20strengthens)[tianweiz07.github.io](https://tianweiz07.github.io/Papers/24-ics-1.pdf#:~:text=three%20representative%20DLT%20schedulers%20and,132%C3%97%20configuration%20tuning%20latency%20reduction)). While these focus on specific domains (deep learning jobs, DLT clusters), they provide performance references.

All experiments are conducted on a simulated cluster of 100 nodes (each with 4 GPUs) to cover large-scale workloads. For reproducibility, our code and workloads will be open-sourced, following prior work practices.

## **6. Related Work**

**Batch and Cluster Scheduling:** Traditional HPC schedulers (e.g. Slurm, Torque) use heuristic policies like FCFS with backfilling. These are simple and fast but inflexible. Heterogeneity-aware schedulers like *Gavel* (OSDI’20) recast policies as optimization problems and adapt allocations to hardware differences, but still rely on fixed objectives.

**RL-based Scheduling:** Recent works apply reinforcement learning to scheduling. Decima (SIGMETRICS’17) uses graph neural nets for DAG job scheduling in data centers. RLScheduler (SC’20) learns HPC batch scheduling policies from traces. DRAS (IPDPS’21) introduces a hierarchical deep RL agent for HPC, incorporating HPC-specific features like job reservation and backfilling. These achieve up to ~45% performance gains over heuristics. MRSch (arXiv’24) uses *multi-objective RL* (direct future prediction) to schedule with multiple resources (CPU, I/O, burst buffer), improving performance by up to 48%. All these highlight RL’s ability to adapt but require extensive training and are specialized per workload. In contrast, Sched-Agent does not need workload-specific retraining; the LLM jumpstarts the policy.

**Task-based Runtime Scheduling:** Systems like StarPU, Legion, and OpenMP runtimes schedule fine-grained tasks via list scheduling and priorities[arxiv.org](https://arxiv.org/html/2404.03226v1#:~:text=characteristics,grained%20tasks%20and). INSPIRIT (arXiv’24) improves such schedulers by introducing new task attributes (“inspiring ability/efficiency”) to guide priorities without domain knowledge[arxiv.org](https://arxiv.org/html/2404.03226v1#:~:text=several%20drawbacks%20to%20determine%20task,effectively%20reducing%20the%20overhead%20for). Task scheduling works typically need domain-specific tuning[arxiv.org](https://arxiv.org/html/2404.03226v1#:~:text=Due%20to%20the%20low%20complexity,Therefore%2C%20when). Sched-Agent targets this problem by having the LLM deduce useful priority rules from workload semantics.

**Multi-Objective and Online Schedulers:** Schedulers that handle multiple goals or online adaptation include Cilantro (OSDI’23), which continuously learns performance-to-resource mappings to meet arbitrary objectives, and RL-based multi-resource schemes. These systems demonstrate the importance of adaptivity; Sched-Agent aims to incorporate such capabilities via the agent’s ability to reconfigure policies online.

**LLM and Auto-Tuning in Systems:** The use of large language models in systems control is emerging. Jobson & Li (AIware’24) showed LLMs can draft conference schedules. Zhang et al. (ICASSP’25) use an LLM to optimize parallel program mappers via a DSL interface, achieving 3.8× speedups over OpenTuner. Their *Agent-System Interface* inspires our DSL usage: abstract low-level code into a structured language, letting the LLM explore high-level mappings. In OS and cluster contexts, LLM-based agent operating systems (e.g. AIOS[arxiv.org](https://arxiv.org/html/2403.16971v1#:~:text=Agent%20Scheduler%3A%20Prioritizes%20and%20schedules,requests%20to%20optimize%20LLM%20utilization)) propose schedulers that prioritize LLM tool calls. Sched-Agent builds on this vision, using an LLM itself as the scheduling agent.

**Autotuning Frameworks:** AutoSched (ICS’24) tunes GPU-training schedulers by generating synthetic workloads and searching config space[tianweiz07.github.io](https://tianweiz07.github.io/Papers/24-ics-1.pdf#:~:text=To%20address%20this%20dilemma%2C%20we,We%20showcase%20how%20AutoSched%20strengthens)[tianweiz07.github.io](https://tianweiz07.github.io/Papers/24-ics-1.pdf#:~:text=three%20representative%20DLT%20schedulers%20and,132%C3%97%20configuration%20tuning%20latency%20reduction), improving schedulers by ~46%. Sched-Agent is related but more general: instead of hand-engineered search, it uses an LLM for initial policy and RL for tuning, applicable beyond DL workloads. Prior auto-tuners (e.g. ytopt) optimize code or DBMS knobs, but Sched-Agent uniquely targets full scheduler logic synthesis.

In summary, Sched-Agent differs by combining *LLM reasoning* with *RL fine-tuning* to generate scheduler logic. It is the first system (to our knowledge) that leverages an LLM agent to both analyze workloads and emit scheduling policies in a DSL, bridging human insight and automated learning.

## **7. Conclusion**

We present **Sched-Agent**, a novel approach to dynamic scheduling that uses an LLM-driven agent to create and configure schedulers on the fly. By exploiting the LLM’s semantic understanding and combining it with RL optimization, Sched-Agent automatically adapts to diverse workloads and achieves better performance than static heuristics or pure RL. Our design answers the key challenges of scheduling in modern environments: it eliminates the need for human expertise in tuning, supports complex and multi-objective goals, and generalizes across domains. In future work we will explore tighter integration (e.g. real hardware prototypes) and richer multi-agent cooperation (multiple LLM agents scheduling different subsystems). We believe this work points the way toward *intelligent, self-driving schedulers* that can keep up with the pace of evolving workloads.

**References:** [9]–[62]

**Citations**

[**A HPC Co-Scheduler with Reinforcement Learning**https://arxiv.org/html/2401.09706v1](https://arxiv.org/html/2401.09706v1#:~:text=utilization,pushing%20for%20new%20scheduling%20techniques)
[**INSPIRIT: Optimizing Heterogeneous Task Scheduling through Adaptive Priority in Task-based Runtime Systems**https://arxiv.org/html/2404.03226v1](https://arxiv.org/html/2404.03226v1#:~:text=characteristics,grained%20tasks%20and)
[**LLM Agent Operating System**https://arxiv.org/html/2403.16971v1](https://arxiv.org/html/2403.16971v1#:~:text=Agent%20Scheduler%3A%20Prioritizes%20and%20schedules,requests%20to%20optimize%20LLM%20utilization)
[**AutoSched: An Adaptive Self-configured Framework for Scheduling Deep Learning Training Workloads**https://tianweiz07.github.io/Papers/24-ics-1.pdf](https://tianweiz07.github.io/Papers/24-ics-1.pdf#:~:text=To%20address%20this%20dilemma%2C%20we,We%20showcase%20how%20AutoSched%20strengthens)
[**AutoSched: An Adaptive Self-configured Framework for Scheduling Deep Learning Training Workloads**https://tianweiz07.github.io/Papers/24-ics-1.pdf](https://tianweiz07.github.io/Papers/24-ics-1.pdf#:~:text=three%20representative%20DLT%20schedulers%20and,132%C3%97%20configuration%20tuning%20latency%20reduction)
[**INSPIRIT: Optimizing Heterogeneous Task Scheduling through Adaptive Priority in Task-based Runtime Systems**https://arxiv.org/html/2404.03226v1](https://arxiv.org/html/2404.03226v1#:~:text=several%20drawbacks%20to%20determine%20task,effectively%20reducing%20the%20overhead%20for)
[**INSPIRIT: Optimizing Heterogeneous Task Scheduling through Adaptive Priority in Task-based Runtime Systems**https://arxiv.org/html/2404.03226v1](https://arxiv.org/html/2404.03226v1#:~:text=Due%20to%20the%20low%20complexity,Therefore%2C%20when)
