\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{xspace}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{pdfpages}

%% Define the \sys command for the system name
\newcommand{\sys}{SchedCP\xspace}
%% Define the \agent command for the sched-agent name
\newcommand{\agent}{sched-agent\xspace}

\title{Project Proposal: Comprehensive Benchmark for Agentic OS Optimization}
\author{Team Members}
\date{}

\begin{document}

\maketitle

\section*{Project Proposal}

\textit{Note: Attached is our accepted workshop paper ``Towards Agentic OS: An LLM Agent Framework for Linux Schedulers'' presented at the MLforSystem Workshop at NeurIPS 2025, which provides the foundation for this extended project.}

\subsection*{1. Motivation}
Consider an 8-year-old playing a 3D game while video chatting—they want their phone to work ``better,'' but cannot articulate what ``better'' means, nor can they configure OS policies. The application developers didn't specify optimization goals to the OS either. Traditional approaches face a fundamental problem: \textbf{you cannot optimize what you cannot define}. High-level metrics (latency, throughput, power) require trade-offs that may not match what applications actually need. This is not merely an optimization problem—it's a \textbf{goal-inference problem}: transforming vague, high-level user intent (``better'') into concrete, optimizable objectives using semantic understanding and domain knowledge.

While our workshop paper shows LLM agents can optimize schedulers (1.79× improvement), it only addresses cases with pre-defined goals (``minimize kernel build time''). Real-world OS management faces two deeper challenges: (1) \textbf{No clear optimization goals}—users and developers cannot specify what they want, and extracting application intent requires semantic analysis beyond static/dynamic monitoring capabilities; (2) \textbf{No systematic evaluation framework}—existing benchmarks measure system performance (SPEC, MLPerf) or code generation quality (HumanEval), but no benchmark evaluates AI agents' ability to \textit{infer goals and optimize systems}, akin to how web browsing benchmarks evaluate agents' ability to use computers. Furthermore, OS policies span scheduling, cache, DVFS, I/O, and network—yet no framework evaluates whether agentic goal-inference generalizes across these domains.

\subsection*{2. Problem Definition and ML Formulation}
We formulate agentic OS management as a \textbf{two-phase problem}: (1) \textbf{Goal-Inference Phase}—given vague user intent, application behavior, and system observations, infer concrete optimization objectives (e.g., ``minimize P99 frame rendering latency for the game while maintaining video call quality''). This requires semantic understanding of application source code, workload characteristics, and domain knowledge about performance trade-offs—capabilities beyond traditional static analysis or dynamic monitoring. (2) \textbf{Policy-Synthesis Phase}—given inferred goals, generate optimized policies. This can use code generation (for nanosecond-latency requirements where ML inference is too slow), configuration tuning, or even training traditional ML models (RL, decision trees) with LLM-selected features and hyperparameters.

Formally, we model this as multi-domain in-context reinforcement learning: \textbf{State space} $\mathcal{S}$ includes user behavior traces, application semantics (from source analysis), and system metrics across CPU/memory/I/O/power/network. \textbf{Action space} $\mathcal{A}$ comprises goal definitions $G$ (phase 1) and policy implementations $P$ (phase 2). \textbf{Reward function} $R(s,a)$ measures goal clarity (is the inferred goal measurable and actionable?) and system performance (does the policy improve user-perceived quality?). The benchmark evaluates: (1) \textbf{Goal-inference accuracy}—do inferred goals match ground truth objectives from expert analysis? (2) \textbf{Optimization effectiveness}—performance vs. baselines (EEVDF, RL-based schedulers, human experts); (3) \textbf{Cross-domain transfer}—does knowledge from scheduler optimization transfer to cache/DVFS/I/O domains? (4) \textbf{Efficiency}—cost and time per optimization.

\subsection*{3. Existing Approaches}
Existing approaches assume \textbf{goals are already known}: (1) \textbf{Traditional adaptive algorithms} (EEVDF, CFS) use heuristics but cannot understand application semantics; (2) \textbf{RL-based optimization} (Decima~\cite{mao2019decima}, FIRM~\cite{qiu2020firm}) requires humans to define state spaces, reward functions, and features—they optimize within human-defined problem spaces but cannot infer what to optimize; (3) \textbf{System tuning tools} (OtterTune~\cite{vanaken2017ottertune}, CherryPick) perform black-box optimization over pre-defined configuration spaces; (4) \textbf{Our \sys framework} enables LLM agents to optimize schedulers but only with explicit goals (``minimize build time'').

For \textbf{benchmarking AI agents}, existing work evaluates either: (1) \textbf{Code generation} (HumanEval, MBPP) without system integration; (2) \textbf{System performance} (SPEC, MLPerf) without AI evaluation; (3) \textbf{Agentic capabilities} (WebArena for web browsing, SWE-bench for software engineering) but not system optimization. \textbf{Critically, no benchmark evaluates goal-inference}: can agents transform vague intent into concrete objectives? This is analogous to the gap before WebArena—we had web browsers and we had LLMs, but no benchmark for ``can LLMs use browsers?'' We have OS subsystems and LLMs, but no benchmark for ``can LLMs infer optimization goals from user behavior?''

\subsection*{4. Existing Datasets and Workloads}
We will build upon and extend existing benchmarks: (1) \textbf{Scheduling workloads} - kernel builds, schbench, batch processing from our workshop paper; (2) \textbf{Memory workloads} - Redis, Memcached with cache-sensitive access patterns; (3) \textbf{I/O workloads} - Filebench, fio with varying read/write ratios; (4) \textbf{Power workloads} - video encoding, ML inference with power caps; (5) \textbf{Composite workloads} - microservice deployments (nginx + database + backend) requiring cross-domain optimization; (6) \textbf{SLO specifications} - industry-standard SLOs from Google SRE, AWS Well-Architected Framework. We will collect ground truth data from manual expert tuning and existing RL baselines across all domains.

\subsection*{5. Novelty}
Our project introduces three key innovations addressing the \textbf{two-sided benchmark problem}:

\textbf{AI/Agentic Side:} (1) \textbf{First goal-inference benchmark} - Unlike code generation benchmarks (HumanEval) or agentic task benchmarks (WebArena), we evaluate agents' ability to \textit{discover optimization objectives} from vague user intent and application behavior. This is fundamentally different from ``writing code is hard, use English instead''—the baseline is ``defining the problem is hard, human experts don't know what to optimize.'' (2) \textbf{Two-phase evaluation protocol} - Separately measure goal-inference quality (do inferred goals match expert analysis?) and policy-synthesis effectiveness (does the implementation achieve the goals?). This lets us diagnose whether failures stem from misunderstanding problems vs. poor implementations. (3) \textbf{Multi-LLM/framework comparison} - Evaluate different models (GPT-4, Claude, Gemini) and agentic frameworks as baselines, similar to how WebArena compares agents' web browsing capabilities.

\textbf{System Side:} (4) \textbf{Comprehensive OS benchmark} - First systematic performance evaluation across scheduling, cache, DVFS, I/O, and network with standardized workloads and metrics. Baselines include traditional adaptive algorithms (EEVDF), RL-based approaches, and human expert tuning. (5) \textbf{Cross-domain transfer evaluation} - Test whether optimization knowledge transfers (e.g., scheduler insights $\rightarrow$ I/O scheduling), revealing whether LLM domain knowledge enables generalization impossible for narrow ML models. (6) \textbf{Real-world deployment scenarios} - Include vague-goal tasks (``make this game better'') alongside clear-goal tasks (``minimize latency $<$10ms''), reflecting actual user needs rather than sanitized benchmarks.

\subsection*{6. Implementation and Verification Plan}
\textbf{System Infrastructure:} Extend \sys to support multiple OS subsystems: (1) \textbf{Cache Policy Engine} - page cache tuning, NUMA policies, THP with performance counters; (2) \textbf{DVFS Controller} - CPU frequency scaling, governor configuration with power monitoring; (3) \textbf{I/O Policy Engine} - block layer schedulers (mq-deadline, BFQ, kyber), filesystem parameters with tracing; (4) \textbf{Network Configurator} - TCP congestion control, buffer sizing with telemetry; (5) \textbf{Cross-domain Coordinator} - enables agents to reason about interactions (e.g., DVFS impacts both scheduler quantum and I/O latency). Total: ~15000 lines across all subsystems.

\textbf{Agentic Benchmark Design (AI Side):} Create tasks at three levels: (1) \textbf{Clear goals} - ``minimize kernel build time'' (baseline, like our workshop paper); (2) \textbf{Vague goals} - ``make 3D game better during video call'' (requires goal inference); (3) \textbf{No explicit goals} - provide only application source code and user behavior traces (agent must discover what matters). \textbf{Ground truth construction:} For each vague-goal task, collect expert analysis defining concrete objectives (e.g., ``maintain 60fps P99 while keeping video call jitter $<$50ms''). Evaluate goal-inference by comparing agent-inferred goals against expert consensus using semantic similarity and measurability metrics. Test multiple LLM agents (GPT-4, Claude, Gemini) and frameworks (AutoGen, LangChain, our \agent) as baselines.

\textbf{System Benchmark Design (System Side):} For each OS domain (scheduling, cache, DVFS, I/O, network), create standardized workloads with: (1) Baseline performance (EEVDF, default configs); (2) RL-based optimization results (when applicable); (3) Human expert tuning (we will conduct this); (4) Cross-domain tasks (e.g., schedule+cache+DVFS coordination). Each task specifies input (workload, vague user intent), expected output (inferred goals, optimized policy), and evaluation metrics (performance improvement, cost, time).

\textbf{Verification:} Heterogeneous testbed: (1) Server (86-core Xeon); (2) Edge (8-core Ultra 7); (3) ARM (Raspberry Pi); (4) Cloud VMs. \textbf{AI-side metrics:} Goal-inference accuracy (semantic match with experts), generalization (transfer across domains), efficiency (\$ and time). \textbf{System-side metrics:} Performance vs. baselines (EEVDF, RL, experts), safety (zero crashes), SLO compliance ($>$95\%). We will publish the complete benchmark suite (code, workloads, ground truth) for reproducibility and community adoption.

\vfill
\noindent\textbf{References:} See bibliography below.

\bibliographystyle{plain}
\bibliography{sample-base}

\clearpage
\section*{Attachment: Workshop Paper}
\noindent The full workshop paper ``Towards Agentic OS: An LLM Agent Framework for Linux Schedulers'' from the MLforSystem Workshop at NeurIPS 2025 is attached below.

\includepdf[pages=-,pagecommand={}]{short.pdf}

\end{document}
