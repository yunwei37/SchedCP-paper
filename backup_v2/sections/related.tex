\section{Related Work}
\label{sec:related}

Machine learning has transformed fundamental system components. Learned indexes~\cite{kraska2018learned} replace B-trees with neural networks, achieving up to 70\% memory savings and 3x lookup performance improvements. In scheduling specifically, Decima~\cite{mao2019decima} uses graph neural networks for datacenter job scheduling, reducing completion time by 40\%, while Firm~\cite{qiu2020firm} addresses multi-resource cluster scheduling with fairness constraints, improving utilization by 25-30\%. MrSch~\cite{zhang2024mrsch} extends these approaches achieving up to 48\% performance improvements. Park~\cite{mao2019park} provides a general platform for applying RL to systems problems. Beyond scheduling, Neo~\cite{marcus2019neo} applies deep RL to database query optimization, achieving 2x performance improvements, while OtterTune~\cite{vanaken2017ottertune} uses Gaussian Process regression to tune database configurations, reaching 94\% of expert DBA performance. However, these approaches share fundamental limitations: they require extensive training (millions of decisions), cannot transfer knowledge between workloads, and operate on statistical patterns without semantic understanding. Recent work demonstrates LLMs' potential in systems optimization. Wang et al.~\cite{wang2024llmsys} show GPT-4 diagnosing 78\% of distributed system problems, and mapper generation~\cite{wei2024mapper} achieves 3.8Ã— speedups for parallel programs, though in constrained domains. Our work uniquely combines semantic understanding with production readiness: unlike RL approaches learning from scratch or auto-tuners optimizing parameters, we leverage LLMs' pre-trained knowledge to generate entirely new scheduling algorithms through an autonomous optimization loop. Built on sched\_ext with eBPF safety guarantees, our self-evolving scheduler library accumulates knowledge across workloads, representing the first AI agents generating production-quality kernel schedulers from natural language requirements.